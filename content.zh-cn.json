{"pages":[{"title":"关于我","text":"本人目前在腾讯就职，主要从事云计算相关，致力于为用户提供最好的云上数据库解决方案，对MySQL、HBase、Redis及其它一些数据平台产品有所研究，热爱技术，热爱生活。本博客主要记录一些个人工作感悟。 Nickname: ballenEmail: zendwind@gmail.comGithub: @zendwind","link":"/zh-cn/about/index.html"},{"title":"categories","text":"","link":"/zh-cn/categories/index.html"}],"posts":[{"title":"【原】Python解析mysqldump文件","text":"最近在做离线数据导入HBase项目，涉及将存储在Mysql中的历史数据通过bulkload的方式导入HBase。由于源数据已经不在DB中，而是以文件形式存储在机器磁盘，此文件是mysqldump导出的格式。如何将mysqldump格式的文件转换成实际的数据文件提供给bulkload作转换，是需要考虑的一个问题。 一、思路我们知道mysqldump导出的文件主要是Insert，数据库表结构定义语句。而要解析的对象也主要是包含INSERT关键字记录，这样我们就把问题转换成如何从dmp文件解析Insert语句。接触过dmp文件的同学应该了解，其INSERT语句的结构，主要包含表名、字段名、字段值, 这里面主要包含几个关键字：INSERT INTO, VALUES。我们要做的就是把Values括号后的字段值给解析出来，这个过程需要考虑VALUES后面包含的是多少行的记录，有可能导出的记录Values后面包含多行对应mysql中存储的记录。 在解析文件过程中，我自然想到用Python来写，因为Python在处理文件方面有很多优势，也比较简单。在处理DMP文件这块，考虑到字段值间是用逗号分割的，在python中正好一个模块可以很好的来处理此类格式 ，即大家很熟悉的CSV模块，在处理CSV类型的文件有很多优势。在这里我们把CSV模块有在解析dmp文件，同时加一些解析逻辑，可以很好解决此类问题。 同时，我们要处理的dmp文件是经过压缩的，并且单个文件都比较大，都是Gigbytes的，在读取时需要注意机器内存大小，不能一次读出所有的数据，python也考虑到此类问题，采用的方法是惰性取值，即在真正使用时才从磁盘中加载相应的文件数据。如果想加块解析，还可以采集多进程或多线程的方法。 二、方法处理流程图如下所示： 代码如下所示： #!/usr/bin/env python import fileinput import csv import sys import gzip # 设定CSV读取的最大容量 csv.field_size_limit(sys.maxsize) def check_insert(line): &quot;&quot;&quot; 返回语句是否以insert into开头，如果是返回true,否则返回false &quot;&quot;&quot; return line.startswith(&apos;INSERT INTO&apos;) or False def get_line_values(line): &quot;&quot;&quot; 返回Insert语句中包含Values的部分 &quot;&quot;&quot; return line.partition(&apos;VALUES &apos;)[2] def check_values_style(values): &quot;&quot;&quot; 保证INSERT语句满足基本的条件，即包含(右括号 &quot;&quot;&quot; if values and values[0] == &apos;(&apos;: return True return False def parse_line(values): &quot;&quot;&quot; 创建csv对象，读取INSERT VALUES 字段值 &quot;&quot;&quot; latest_row = [] reader = csv.reader([values], delimiter=&apos;,&apos;, doublequote=False, escapechar=&apos;\\\\&apos;, quotechar=&quot;&apos;&quot;, strict=True ) for reader_row in reader: for column in reader_row: # 判断字段值是否为空或为NULL if len(column) == 0 or column == &apos;NULL&apos;: latest_row.append(&quot;&quot;) continue # 判断字段开头是否以（开头,如果是则说明此VALUES后面不只包含一行数据，可能有多行，要分别解析 if column[0] == &quot;(&quot;: new_row = False if len(latest_row) &gt; 0: #判断行是否包含）,如果包含则说明一行数据完毕 if latest_row[-1][-1] == &quot;)&quot;: # 移除） latest_row[-1] = latest_row[-1][:-1] if latest_row[-1] == &quot;NULL&quot;: latest_row[-1] = &quot;&quot; new_row = True # 如果是新行，则打印该行 if new_row: line=&quot;}}}{{{&quot;.join(latest_row) print &quot;%s&quot; % line latest_row = [] if len(latest_row) == 0: column = column[1:] latest_row.append(column) # 判断行结束符 if latest_row[-1][-2:] == &quot;);&quot;: latest_row[-1] = latest_row[-1][:-2] if latest_row[-1] == &quot;NULL&quot;: latest_row[-1] = &quot;&quot; line=&quot;}}}{{{&quot;.join(latest_row) print &quot;%s&lt;{||}&gt;&quot; % line def main(): filename=sys.argv[1] try: #惰性取行 with gzip.open(filename,&quot;rb&quot;) as f: for line in f: if check_insert(line): values = get_line_values(line) if check_values_style(values): parse_line(values) except KeyboardInterrupt: sys.exit(0) if __name__ == &quot;__main__&quot;: main() 三、总结总的说来，主要是利用Python的CSV模块来解析DMP文件的INSERT语句，如果DMP文件不规整，可能还是有些问题。对于dmp文件很大情况，也是需要考虑解析时间效率问题，可以考虑增加多进程或多线程机制。 版权声明：本文为博主原创文章，未经博主允许不得转载。","link":"/2017/10/15/zh-cn/python-read-dump/"},{"title":"【原】HBase读写异常问题总结","text":"国庆期间，HBase集群出现一次比较严重的问题，故障期间，业务方反馈查询大量超时，由于涉及重点业务查询，影响也是比较大的。下面复盘一下问题发生过程并作相应分析 一、问题描述10月6号傍晚从18:00点开始，业务陆续反馈查询HBase历史数据异常，出现大量超时，下面是当时的业务成功率曲线。整个过程经历了5次成功率下降的阶段，这是之前未出现过的。 在查看重点业务组所在机器的写监控曲线，故障期间重点表基本不可写： 同时重点表的读也是异常的，基本不可读： 二、原因分析2.1 采集分析查看数据采集曲线，未看到有表数据发生延迟采集或DB数据源故障情况，排除采集异常。 2.2 消费分析查看数据消费曲线，发现重点业务的所有表消费曲线延迟严重，消费者本身无报错产生，查看消费日志写得很慢，既然消费未报错又写得慢，问题应该出现在HBase端。 2.3 HBase分析2.3.1 第一次&amp;第二次异常原因分析根据经验，HBase读写异常很大程度上和机器异常有关。首先，找出重点业务所在的机器列表，并根据所采集的监控曲线，看了下这些机器的处理时耗，如下图所示：从图中，可看出在故障期间，这些机器的处理时耗异常，是所有机器都异常还是只有个别机器处理异常呢，继续从上面图查看分布状态图，如下图所示：从图可以看出，确实有一台机器的处理时耗比其它机器都高，找到异常机器后，从tnm2上看了下单机性能情况，如下图所示：从图中，可以看出，异常机器在18:00附近各项指标也是异常的，磁盘IO、CPU负载、SWAP内存都表现异常，说明机器确实存在问题。那是什么原因导致的机器负载异常呢，继续登陆机器查看日志情况，在18:00附近左右，找到如下一个异常：从多个异常描述中，都发现一个共性现象，就是集中在t_tcbankroll_list_auid_201707这个表上，看日志信息是这个表在作合并，查看了下合并任务，发现确实在这天有一条合并定时任务，合并任务主要是合并3个月前的表，正好是上述7月份的表，是不是合并存在异常呢，继续看了下监控中关于该异常IP的合并队列,发现在18:00合并队列高达59000，即有几万个合并任务等着被执行，之后合并队列表现异常，机器读写也异常，说明确实合并有问题。为什么合并会有问题呢？进一步看了下异常表的region的大小，发现大小严重不均，集中在两个端，8G和20G左右。合并出问题应该是出在合并20G大小的region时出现的问题。是在合并哪个Region的时候出的问题呢，再看下图7中异常机器的日志，发现324c091cfd8e944507ae645ddeda78e2这个region合并时出的问题，同时这个region的大小在图9中正好属于20G那个范围的，验证了上述猜想。那为什么合并大Region过程出现问题了呢，再从18:00异常时间点附近看了下日志，如下所示 2017-10-06 18:22:36,280 WARN [regionserver60020] util.Sleeper: We slept 24555ms instead of 3000ms, this is likely due to a long garbage collecting pause and it&apos;s usually bad, see，http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired 2017-10-06 18:22:36,280 WARN [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 22680ms GC pool &apos;ParNew&apos; had collection(s): count=1 time=247ms GC pool &apos;ConcurrentMarkSweep&apos; had collection(s): count=1 time=22712ms 这种迹象很明显就是在作FullGC，频繁的GC过程阻塞了读写进程， 在这个时间段内，看了下JVM堆内存的使用量，发现异常的高，如下所示：了解FullGC的人都知道，FullGC期间，集群读写会被堵塞，如图7所示客户端连接会出现超时异常，再加上我们机器本身性能比较差，导致FullGC过程中，机器IO、网络、CPU、内存和SWAP等都出现异常，阻塞了集群读写，影响了重点业务的查询。知道啥原因后，直接把机器剔除停掉regionserver进程，业务读写开始恢复正常。 好景不长，只过了不到20min, 第二次异常突然降临，在18点56时，业务查询成功率又开始降低。回想一下第一次故障，只是把当时有异常的机器剔除，但没考虑的是异常表的合并任务并未终结，只是把合并异常的任务转移到其它机器，重点业务组机器并未终止合并，即其它机器的负载并未得到有效缓解，看监控曲线发现，此时另一台机器出现异常，如图10所示，该机集群处理时耗明显高于其它机器，和第一次样，把异常机器剔除后业务恢复正常。 2.3.2 第三次&amp;第四次&amp;第五次故障分析前面2次的故障还好，都还是单机异常所致，后面出现的两次异常却是多机同时出现异常，此时单纯剔除单台机器已无法解决问题，因重点业务组所在机器数量比较少，再重新扩机器进去也不太可行，如果都剔除那读写就完全中止了，影响更大。上面说到，其实前两次故障都没解决合并异常的本质问题，合并还在继续，合并队列也维持在一个高位，此时对整个机器组来说压力都是非常大的，机器资源大量被用于作合并，导致其它操作无法有效获取资源。只能想其它办法解决。既然合并队列一直在高位，有什么办法可以降低合并队列长度呢，社区HBase-17928补丁给我们带来了福音，它要解决的就是当表合并异常或在很长时间合并都未完成的场景。使用方式如下： clear_queues &apos;SERVERNAME&apos;, [&apos;QUEUE_NAME1&apos;,&apos;QUEUE_NAME2&apos;] 其中，SERVERNAME表示regionserver，形式如ip,port,timestamp，QUEUE_NAME表示队列的长度，可以为short,long。对于我们来说，采用的是直接清理到机器的合并队列，即如果机器10.x.x.x有问题，清理如下： clear_queues &apos;10.x.x.x,60020,1491511804929&apos; 在清理部分合并队列过长的机器之后，集群读写恢复了部分，但还未全部恢复，导致了第四次业务查询还是受影响，因为清理时没把所有机器清理干净，所以异常还未完全解决。第四次故障之后，对剩下的机器也全部清理了一遍队列，同时还作了一个操作，就是把异常表移出重点业务组，让合并在组外的机器进行，这样能尽量避免异常表对其它表影响。 在做完上述动作之后，隔了不到10min, 第5次故障又如期而至，充分验证了墨菲定律，不想发生的偏要发生。第5次故障是什么原因呢，从监控曲线上面看，根据第一次和第二次的故障分析，总结出此次也是因单机故障引起的，首先做的就是剔除该异常机器，但因负载问题，一直登录异常，耽误了异常恢复原因，等剔除后业务终于开始恢复正常。 在观察一段时间后，整个重点业务组暂时运行稳定，此时因踢掉了多台机器，组内的10几台机器的region已经分布不均，有潜在风险，需要考虑把之前剔除机器上的region还原。正常的还原方法是，如果regionserver剔除是按正规流程剔除的话，会记录该机器上有什么region，然后机器恢复后把region再move回来即可；不过，像我们这种情况，属于紧急剔除，踢之前没考虑记录region信息，需要考虑从其它渠道来恢复信息，还有一种方法是从日志恢复，日志恢复方法就是从master上去找剔除时间点开始往后带关键字如Transition和Offline等关键字的日志，然后找出region move回到原机器即可。 三、故障梳理经过上述一系列的分析和采取的一系列措施，故障得以恢复，整个故障原因总结如下： 合并任务异常，触发机器频繁GC，机器负载异常 GC阻塞了DFSClient的读写请求发送 HDFS服务端报大量的读写Socket连接超时并关闭client请求连接 故障措施总结： 剔除异常机器regionserver进程 清理异常机器的合并队列 恢复region回原机器 四、故障反思此次故障引发的连锁反应之前也没遇到过，耽误了很多时间，也侧面反应出自己对这些异常缺乏足够的认识，通过此次排查也让自己对regionserver异常有一个更清楚的了解，后续在运维过程中需要更多关注以下一些点： 出现异常，以业务恢复为先，先保留事故现场日志，该剔除的剔除 监控完善，一直在说监控问题，但监控事项总有些纰漏，需在这块优化加强，尤其是巡检机制 业务重点表隔离细化，虽然已经做了相应隔离，但分级还是不够细化，需针对重点业务再作分级隔离","link":"/2017/10/11/zh-cn/hbase-read-write-exception-summary/"},{"title":"【原】Hadoop HA机制学习","text":"最近在内部分享过一次关于Hadoop技术主题的演讲，由于接触时间不长，很多技术细节认识不够，也没讲清楚，作为一个技术人员，本着追根溯源的精神，还是有必要吃透，也为自己的工作沉淀一些经验总结。网上关于Hadoop HA的资料多集中于怎么搭建HA，对于HA为什么要这么做描述甚少，所以本文对于HA是如何搭建的暂不介绍，主要是介绍HA是怎么运作，QJM又是怎么发挥功效的。 一、Hadoop 系统架构1.1 Hadoop1.x和Hadoop2.x 架构在介绍HA之前，我们先来看下Hadoop的系统架构，这对于理解HA是至关重要的。Hadoop 1.x之前，其官方架构如图1所示: 从图中可看出，1.x版本之前只有一个Namenode,所有元数据由惟一的Namenode负责管理,可想而之当这个NameNode挂掉时整个集群基本也就不可用。Hadoop 2.x的架构与1.x有什么区别呢。我们来看下2.x的架构： 2.x版本中，HDFS架构解决了单点故障问题，即引入双NameNode架构，同时借助共享存储系统来进行元数据的同步，共享存储系统类型一般有几类，如：Shared NAS+NFS、BookKeeper、BackupNode 和 Quorum Journal Manager(QJM)，上图中用的是QJM作为共享存储组件，通过搭建奇数结点的JournalNode实现主备NameNode元数据操作信息同步。Hadoop的元数据包括哪些信息呢，下面介绍下关于元数据方面的知识。 1.2 Hadoop 2.x元数据Hadoop的元数据主要作用是维护HDFS文件系统中文件和目录相关信息。元数据的存储形式主要有3类：内存镜像、磁盘镜像(FSImage)、日志(EditLog)。在Namenode启动时，会加载磁盘镜像到内存中以进行元数据的管理，存储在NameNode内存；磁盘镜像是某一时刻HDFS的元数据信息的快照，包含所有相关Datanode节点文件块映射关系和命名空间(Namespace)信息，存储在NameNode本地文件系统；日志文件记录client发起的每一次操作信息，即保存所有对文件系统的修改操作，用于定期和磁盘镜像合并成最新镜像，保证NameNode元数据信息的完整，存储在NameNode本地和共享存储系统(QJM)中。 如下所示为NameNode本地的EditLog和FSImage文件格式，EditLog文件有两种状态： inprocess和finalized, inprocess表示正在写的日志文件，文件名形式:edits_inprocess_[start-txid]，finalized表示已经写完的日志文件,文件名形式：edits_[start-txid]_[end-txid]； FSImage文件也有两种状态, finalized和checkpoint， finalized表示已经持久化磁盘的文件，文件名形式: fsimage_[end-txid], checkpoint表示合并中的fsimage, 2.x版本checkpoint过程在Standby Namenode(SNN)上进行，SNN会定期将本地FSImage和从QJM上拉回的ANN的EditLog进行合并，合并完后再通过RPC传回ANN。data/hbase/runtime/namespace├── current│ ├── VERSION│ ├── edits_0000000003619794209-0000000003619813881│ ├── edits_0000000003619813882-0000000003619831665│ ├── edits_0000000003619831666-0000000003619852153│ ├── edits_0000000003619852154-0000000003619871027│ ├── edits_0000000003619871028-0000000003619880765│ ├── edits_0000000003619880766-0000000003620060869│ ├── edits_inprogress_0000000003620060870│ ├── fsimage_0000000003618370058│ ├── fsimage_0000000003618370058.md5│ ├── fsimage_0000000003620060869│ ├── fsimage_0000000003620060869.md5│ └── seen_txid└── in_use.lock上面所示的还有一个很重要的文件就是seen_txid,保存的是一个事务ID，这个事务ID是EditLog最新的一个结束事务id，当NameNode重启时，会顺序遍历从edits_0000000000000000001到seen_txid所记录的txid所在的日志文件，进行元数据恢复，如果该文件丢失或记录的事务ID有问题，会造成数据块信息的丢失。 HA其本质上就是要保证主备NN元数据是保持一致的，即保证fsimage和editlog在备NN上也是完整的。元数据的同步很大程度取决于EditLog的同步，而这步骤的关键就是共享文件系统，下面开始介绍一下关于QJM共享存储机制。 二、QJM原理2.1 QJM背景在QJM出现之前，为保障集群的HA，设计的是一种基于NAS的共享存储机制，即主备NameNode间通过NAS进行元数据的同步。该方案有什么缺点呢，主要有以下几点： 定制化硬件设备：必须是支持NAS的设备才能满足需求 复杂化部署过程：在部署好NameNode后，还必须额外配置NFS挂载、定制隔离脚本，部署易出错 简陋化NFS客户端：Bug多，部署配置易出错，导致HA不可用 所以对于替代方案而言，也必须解决NAS相关缺陷才能让HA更好服务。即设备无须定制化，普通设备即可配置HA，部署简单，相关配置集成到系统本身，无需自己定制，同时元数据的同步也必须保证完全HA，不会因client问题而同步失败。 2.2 QJM原理2.2.1 QJM介绍QJM全称是Quorum Journal Manager, 由JournalNode（JN）组成，一般是奇数点结点组成。每个JournalNode对外有一个简易的RPC接口，以供NameNode读写EditLog到JN本地磁盘。当写EditLog时，NameNode会同时向所有JournalNode并行写文件，只要有N/2+1结点写成功则认为此次写操作成功，遵循Paxos协议。其内部实现框架如下： 从图中可看出，主要是涉及EditLog的不同管理对象和输出流对象，每种对象发挥着各自不同作用： FSEditLog：所有EditLog操作的入口 JournalSet: 集成本地磁盘和JournalNode集群上EditLog的相关操作 FileJournalManager: 实现本地磁盘上 EditLog 操作 QuorumJournalManager: 实现JournalNode 集群EditLog操作 AsyncLoggerSet: 实现JournalNode 集群 EditLog 的写操作集合 AsyncLogger：发起RPC请求到JN，执行具体的日志同步功能 JournalNodeRpcServer：运行在 JournalNode 节点进程中的 RPC 服务，接收 NameNode 端的 AsyncLogger 的 RPC 请求。 JournalNodeHttpServer：运行在 JournalNode 节点进程中的 Http 服务，用于接收处于 Standby 状态的 NameNode 和其它 JournalNode 的同步 EditLog 文件流的请求。 下面具体分析下QJM的读写过程。 2.2.2 QJM 写过程分析上面提到EditLog，NameNode会把EditLog同时写到本地和JournalNode。写本地由配置中参数dfs.namenode.name.dir控制，写JN由参数dfs.namenode.shared.edits.dir控制，在写EditLog时会由两个不同的输出流来控制日志的写过程，分别为：EditLogFileOutputStream(本地输出流)和QuorumOutputStream(JN输出流)。写EditLog也不是直接写到磁盘中，为保证高吞吐，NameNode会分别为EditLogFileOutputStream和QuorumOutputStream定义两个同等大小的Buffer，大小大概是512KB，一个写Buffer(buffCurrent)，一个同步Buffer(buffReady)，这样可以一边写一边同步，所以EditLog是一个异步写过程，同时也是一个批量同步的过程，避免每写一笔就同步一次日志。这个是怎么实现边写边同步的呢，这中间其实是有一个缓冲区交换的过程，即bufferCurrent和buffReady在达到条件时会触发交换，如bufferCurrent在达到阈值同时bufferReady的数据又同步完时，bufferReady数据会清空，同时会将bufferCurrent指针指向bufferReady以满足继续写，另外会将bufferReady指针指向bufferCurrent以提供继续同步EditLog。上面过程用流程图就是表示如下： 这里有一个问题，既然EditLog是异步写的，怎么保证缓存中的数据不丢呢,其实这里虽然是异步,但实际所有日志都需要通过logSync同步成功后才会给client返回成功码，假设某一时刻NameNode不可用了，其内存中的数据其实是未同步成功的，所以client会认为这部分数据未写成功。第二个问题是，EditLog怎么在多个JN上保持一致的呢。下面展开介绍。 1.隔离双写：在ANN每次同步EditLog到JN时，先要保证不会有两个NN同时向JN同步日志。这个隔离是怎么做的。这里面涉及一个很重要的概念Epoch Numbers，很多分布式系统都会用到。Epoch有如下几个特性： 当NN成为活动结点时，其会被赋予一个EpochNumber 每个EpochNumber是惟一的，不会有相同的EpochNumber出现 EpochNumber有严格顺序保证，每次NN切换后其EpochNumber都会自增1，后面生成的EpochNumber都会大于前面的EpochNumber QJM是怎么保证上面特性的呢，主要有以下几点： 第一步，在对EditLog作任何修改前，QuorumJournalManager(NameNode上)必须被赋予一个EpochNumber 第二步， QJM把自己的EpochNumber通过newEpoch(N)的方式发送给所有JN结点 第三步， 当JN收到newEpoch请求后，会把QJM的EpochNumber保存到一个lastPromisedEpoch变量中并持久化到本地磁盘 第四步， ANN同步日志到JN的任何RPC请求（如logEdits(),startLogSegment()等），都必须包含ANN的EpochNumber 第五步，JN在收到RPC请求后，会将之与lastPromisedEpoch对比，如果请求的EpochNumber小于lastPromisedEpoch,将会拒绝同步请求，反之，会接受同步请求并将请求的EpochNumber保存在lastPromisedEpoch 这样就能保证主备NN发生切换时，就算同时向JN同步日志，也能保证日志不会写乱，因为发生切换后，原ANN的EpochNumber肯定是小于新ANN的EpochNumber，所以原ANN向JN的发起的所有同步请求都会拒绝，实现隔离功能，防止了脑裂。 2. 恢复in-process日志为什么要这步呢，如果在写过程中写失败了，可能各个JN上的EditLog的长度都不一样，需要在开始写之前将不一致的部分恢复。恢复机制如下： 1 ANN先向所有JN发送getJournalState请求； 2 JN会向ANN返回一个Epoch（lastPromisedEpoch)； 3 ANN收到大多数JN的Epoch后，选择最大的一个并加1作为当前新的Epoch，然后向JN发送新的newEpoch请求，把新的Epoch下发给JN； 4 JN收到新的Epoch后，和lastPromisedEpoch对比，若更大则更新到本地并返回给ANN自己本地一个最新EditLogSegment起始事务Id,若小则返回NN错误； 5 ANN收到多数JN成功响应后认为Epoch生成成功，开始准备日志恢复； 6 ANN会选择一个最大的EditLogSegment事务ID作为恢复依据，然后向JN发送prepareRecovery； RPC请求，对应Paxos协议2p阶段的Phase1a，若多数JN响应prepareRecovery成功，则可认为Phase1a阶段成功； 7 ANN选择进行同步的数据源，向JN发送acceptRecovery RPC请求，并将数据源作为参数传给JN。 8 JN收到acceptRecovery请求后，会从JournalNodeHttpServer下载EditLogSegment并替换到本地保存的EditLogSegment，对应Paxos协议2p阶段的Phase1b，完成后返回ANN请求成功状态。 9 ANN收到多数JN的响应成功请求后，向JN发送finalizeLogSegment请求，表示数据恢复完成，这样之后所有JN上的日志就能保持一致。 数据恢复后，ANN上会将本地处于in-process状态的日志更名为finalized状态的日志，形式如edits_[start-txid]_[stop-txid]。 3.日志同步这个步骤上面有介绍到关于日志从ANN同步到JN的过程,具体如下： 1 执行logSync过程，将ANN上的日志数据放到缓存队列中 2 将缓存中数据同步到JN，JN有相应线程来处理logEdits请求 3 JN收到数据后，先确认EpochNumber是否合法，再验证日志事务ID是否正常，将日志刷到磁盘，返回ANN成功码 4 ANN收到JN成功请求后返回client写成功标识，若失败则抛出异常 通过上面一些步骤，日志能保证成功同步到JN，同时保证JN日志的一致性，进而备NN上同步日志时也能保证数据是完整和一致的。 2.2.3 QJM读过程分析这个读过程是面向备NN(SNN)的，SNN定期检查JournalNode上EditLog的变化，然后将EditLog拉回本地。SNN上有一个线程StandbyCheckpointer，会定期将SNN上FSImage和EditLog合并，并将合并完的FSImage文件传回主NN（ANN）上，就是所说的Checkpointing过程。下面我们来看下Checkpointing是怎么进行的。在2.x版本中，已经将原来的由SecondaryNameNode主导的Checkpointing替换成由SNN主导的Checkpointing。下面是一个CheckPoint的流向图: 总的来说，就是在SNN上先检查前置条件，前置条件包括两个方面：距离上次Checkpointing的时间间隔和EditLog中事务条数限制。前置条件任何一个满足都会触发Checkpointing，然后SNN会将最新的NameSpace数据即SNN内存中当前状态的元数据保存到一个临时的fsimage文件( fsimage.ckpt）然后比对从JN上拉到的最新EditLog的事务ID，将fsimage.ckpt_中没有，EditLog中有的所有元数据修改记录合并一起并重命名成新的fsimage文件，同时生成一个md5文件。将最新的fsimage再通过HTTP请求传回ANN。通过定期合并fsimage有什么好处呢，主要有以下几个方面： 可以避免EditLog越来越大，合并成新fsimage后可以将老的EditLog删除 可以避免主NN（ANN）压力过大，合并是在SNN上进行的 可以保证fsimage保存的是一份最新的元数据，故障恢复时避免数据丢失 三、主备切换机制要完成HA，除了元数据同步外，还得有一个完备的主备切换机制，Hadoop的主备选举依赖于ZooKeeper。下面是主备切换的状态图： 从图中可以看出，整个切换过程是由ZKFC来控制的，具体又可分为HealthMonitor、ZKFailoverController和ActiveStandbyElector三个组件。 ZKFailoverController: 是HealthMontior和ActiveStandbyElector的母体，执行具体的切换操作 HealthMonitor: 监控NameNode健康状态，若状态异常会触发回调ZKFailoverController进行自动主备切换 ActiveStandbyElector: 通知ZK执行主备选举，若ZK完成变更，会回调ZKFailoverController相应方法进行主备状态切换 在故障切换期间，ZooKeeper主要是发挥什么作用呢，有以下几点： 失败保护：集群中每一个NameNode都会在ZooKeeper维护一个持久的session,机器一旦挂掉，session就会过期，故障迁移就会触发 Active NameNode选择：ZooKeeper有一个选择ActiveNN的机制，一旦现有的ANN宕机，其他NameNode可以向ZooKeeper申请排他成为下一个Active节点 防脑裂： ZK本身是强一致和高可用的，可以用它来保证同一时刻只有一个活动节点 那在哪些场景会触发自动切换呢，从HDFS-2185中归纳了以下几个场景： ActiveNN JVM奔溃：ANN上HealthMonitor状态上报会有连接超时异常，HealthMonitor会触发状态迁移至SERVICE_NOT_RESPONDING, 然后ANN上的ZKFC会退出选举，SNN上的ZKFC会获得Active Lock, 作相应隔离后成为Active结点。 ActiveNN JVM冻结：这个是JVM没奔溃，但也无法响应，同奔溃一样，会触发自动切换。 ActiveNN 机器宕机：此时ActiveStandbyElector会失去同ZK的心跳，会话超时，SNN上的ZKFC会通知ZK删除ANN的活动锁，作相应隔离后完成主备切换。 ActiveNN 健康状态异常： 此时HealthMonitor会收到一个HealthCheckFailedException，并触发自动切换。 Active ZKFC奔溃：虽然ZKFC是一个独立的进程，但因设计简单也容易出问题，一旦ZKFC进程挂掉，虽然此时NameNode是OK的，但系统也认为需要切换，此时SNN会发一个请求到ANN要求ANN放弃主结点位置，ANN收到请求后，会触发完成自动切换。 ZooKeeper奔溃：如果ZK奔溃了，主备NN上的ZKFC都会感知断连，此时主备NN会进入一个NeutralMode模式，同时不改变主备NN的状态，继续发挥作用，只不过此时，如果ANN也故障了，那集群无法发挥Failover, 也就不可用了，所以对于此种场景，ZK一般是不允许挂掉到多台，至少要有N/2+1台保持服务才算是安全的。 五、总结上面介绍了下关于HadoopHA机制，归纳起来主要是两块：元数据同步和主备选举。元数据同步依赖于QJM共享存储，主备选举依赖于ZKFC和Zookeeper。整个过程还是比较复杂的，如果能理解Paxos协议，那也能更好的理解这个。希望这篇文章能让大家更深入了解关于HA方面的知识。 六、参考文献[1.] http://zh.hortonworks.com/blog/hdfs-metadata-directories-explained/[2.] https://blog.cloudera.com/blog/2014/03/a-guide-to-checkpointing-in-hadoop/[3.] https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/","link":"/2017/10/08/zh-cn/hadoop-ha-qjm/"},{"title":"【原】Hadoop小文件存储方案","text":"近来，业务部门因历史原因，希望对现存的图片、对账等历史文件进行改造，由原先的单机存储改成分布式存储便于管理和维护，目前组内也在大力推广HDFS在部门的应用 ，所以在此背景下，调研了目前关于HDFS的文件存储方案，本文会着重从小文件需求入手，分析目前各种现有小文件存储的状况及各自使用的场景。 HDFS总体架构在介绍文件存储方案之前，我觉得有必要先介绍下关于HDFS存储架构方面的一些知识，在对架构有初步了解后，才会明白为什么要单独针对小文件展开介绍，小文件存储和其它文件存储区别在什么地方。 这里我只是就Hadoop生态中的存储层展开介绍，对于其它部分本文暂未描述。众所周知，HDFS是目前非常流行的分布式文件存储系统，其逻辑架构如下图所示： HDFS也是典型的Master/Slave结构，其中，Master相当于Namenode，Slave相当于Datanode。 Namenode 负责元数据管理，维护文件和目录树，响应Client请求；Datanode负责实际数据存储。至于什么是元数据，是怎么管理的，可参考我的另一篇文章Hadoop的HA机制。 Block是文件块，HDFS中是以Block为单位进行文件的管理的，一个文件可能有多个块，每个块默认是3个副本，这些块分别存储在不同机器上。块与文件之前的映射关系会定时上报Namenode。HDFS中一个块的默认大小是64M，其大小由参数dfs.block.size控制。这里面先引申几个问题出来： 问题1：块大小要怎么设置为一个合理值，过大设置和过小设置有什么影响？ 问题2：如果一个文件小于所设置的块大小，实际占用空间会怎样？ 问题3：一个Namenode最多能管理多少个块，什么时候会达到瓶颈？ 针对这些问题，后面会展开介绍，这里还是先关注下架构方面。针对块方面，有几个单位概念需要弄清楚： Block、Packet和Chunk。Block上面有描述，Packet和Chunk如下： Packet: 其比块要小很多，可以理解为Linux操作系统最小盘块概念，一般为64KB，由参数dfs.write.packet.size控制，是client向Datanode写入数据的粒度，即client向Datanode写数据时不是一次以Block为单位写的，而是被分成若干Packet，放入pipeline顺序追加写入到Block中，示意图如下： Chunk: 比Packet更小，是针对Packet数据校验粒度来设计的，一般是512B,由参数io.bytes.per.checksum控制，同时还带有一个4B的校验值，所以可以认为一个Chunk是516B 上面说到Chunk是针对数据校验的，那一个Packet有多少个chunk校验呢，如果Packet默认是64KB, 那计算公式为：chunk个数=64KB/516B=128。也就是对于一个Packet来说，数据值与校验值比例大概为128:1, 对于一个块来说，假设是64M，会对应512KB的校验文件。 Packet的示意图中还一个Header信息，实际存储的是Packet的元数据信息，包括Packet在block中的offset, 数据长度，校验编码等。 HDFS写流程了解块相关概念后，再介绍下HDFS的写入流程，如下图所示： client向Namenode发起写文件RPC请求； Namenode检查要写的文件是否已存在元数据中，存在则拒绝写入；同时检查写入用户权限，如无权限也拒绝写入；若文件不存在且有权限写入，则Namenode会创建一条文件记录，响应client端允许写入文件； client根据文件大小分成若干块，并在Namenode中申请块所存放的Datanode位置，如果是3副本存储，则Namenode会选择3台符合条件的结点放到结点队列中；client实际向Datanode写数据时是以Packet为单位来写到Block的，这里面会涉及两个队列，分别为:data packet队列和ack packet队列，Packet会同时入数据队列和ack队列； 通过DataStreamer对象将数据写入pipeline中的第一个Datanode,并依次写入到其它两个结点；当三个结点packet都写成功后，会将packets 从ack queue中删除； 写操作完成后，client调用close()关闭写操作，并通知Namenode关闭写操作，至此，整个写操作完成。 packet写入流示意图如下所示： HDFS读流程HDFS的读流程比较简单，流程程如下所示： client 向Namenode发起读文件RPC请求； Namenode返回相应block所在datanode的位置信息； client通过位置信息调用FSDataInputStream API的Read方法从datanode中并行读取block信息，如图中4和5所示，选择block的其中一副本返回client。 HDFS块信息介绍在对HDFS的读写流程有一个基础了解后，下面针对文件块存储相关内容展开介绍。了解块的设计、存储和元数据相关知识对于设计小文件存储方案也至关重要。 HDFS块设计原则有人可能会问，集群存储有大文件也有小文件，那块大小该如何设计呢，这里应该要考虑2个准则： 1.减少内存占用：对于Namenode来说，单机内存毕竟有限，文件块越多，元数据信息越大，占用内存越多，如果文件数量级很大的话，单机将无法管理； 2.减少硬盘寻道时间： 数据块在硬盘为连续存储，对于普通SATA盘，随机寻址较慢， 如果块设置过小，一个文件的块总数会越多，意味着硬盘寻址时间会加长，自然吞吐量无法满足要求；如果块设置过大，一方面对于普通盘来说IO性能也比较差，加载时会很慢，另一方面，块过大，对于多副本来说，在副本出问题时，系统恢复时间越长。 所以设置合理的块大小也很重要，一般来说根据集群的需求来设定，比如对于使用到HBase的场景，一般数据量会比较大，块不宜设置太小，参考值一般为128MB或256MB，这样能尽量避免频繁块刷写和块元数据信息的膨胀；对于存储小文件的场景，如图片，块可设置成默认64MB大小，一个块中存储多个图片文件，后面会详细介绍。 HDFS块存储原则块在HDFS中是怎么存储的呢，上面有提到多副本机制，即一个块在HDFS中是根据dfs.replication参数所设置的值来确定副本数的，默认为3。三个副本是随机存储三台数据结点Datanode上，三个结点的选取遵循机架感知策略，通过topology.script.file.name来设置， 如果配置中未配置机架感知，Namenode是无法知道机房网络拓扑，所以会随机选取3台结点进行块存储，如果设置了机架感知，则在存储时会在同机架存储2副本，不同机架放第3个副本，这样一旦一个机架出现问题，还能保证一个副本是可用的。 如果一个文件只有几K，且小于HDFS块大小，实际在HDFS占用的空间会是多少呢？答案是文件大小即为实际占用空间，对于几K的文件实际占用的空间大小也为几K，不会占用一个块空间。 HDFS块元数据信息上面提到，在存储的文件数量级很大时，单机Namenode内存消耗会急剧增大，易触发单机瓶颈，那么到底一个Namenode可以管理多少量级的元数据呢，其实这个可以有一个公式来初略估算。这里首先要了解一个概念，元数据包括哪些，正常元数据包括三个部分：文件、目录和块。这三部分在元数据中各占用多少空间呢，下面是一个初略的计算： 单条元数据大小：文件约250B，目录约290B，块约368B(152B+72*副本数3) 集群元数据总条数：文件数约10000个，目录约5000个，块约20000个 总占用内存大小： 250B10000+290B5000+368B*20000=10.78M 实际内存消耗会比这多，因为还有其它一些信息需要存储，总体内存消耗可根据上述公式来估算，这样你就知道你集群Namenode能承受多少文件，目录和块元数据信息的存储。也能及时发现内存瓶颈，做到精细化监控运营管理。 上述介绍的三个方面也分别解答了上面提到的三个问题，具体细节这里也不过多展开。下面正式展开对小文件存储方面的介绍 HDFS小文件存储方案针对小文件问题，HDFS自身也有考虑这种场景，目前已知的主要有三种方案来实现这种存储，分别如下： HAR SequenceFile CombinedFile HAR存储方案HAR熟称Hadoop归档文件，文件以*.har结尾。归档的意思就是将多个小文件归档为一个文件，归档文件中包含元数据信息和小文件内容，即从一定程度上将Namenode管理的元数据信息下沉到Datanode上的归档文件中，避免元数据的膨胀。 归档文件是怎么生成的呢，主要还是依赖于MapReduce原理将小文件内容进行归并。归档文件的大概组成如下所示： 图中，左边是原始小文件，右边是har组成。主要包括：_masterindex、_index、part-0...part-n。其中_masterindex和_index就是相应的元数据信息，part-0…part-n就是相应的小文件内容。实际在集群中的存储结构如下： 通过hadoop archive命令创建归档文件，-archiveName指定文件名, -p指定原文件路径，-r指定要归档的小文件,最后指定hdfs中归档文件存放路径，如下所示： 创建后，会在/usr/archive目录下生成test.har目录，这里大家可能会有疑惑，上面不是说Har是一个文件吗，这里怎么又是目录了呢，其实我们所说的归档文件是逻辑上的概念，而实际的har是一个目录，是一个物理存储概念，所以大家只要记住在实际存储时生成的Har实际上是一个目录就行了。这个目录中会存放元数据，实际文件内容。如下图所示，_index文件的每一行表示的是小文件在part开头的映射关系，包括起始和结束位置，是在哪个part文件等，这样在读取har中的小文件时，根据offset位置可直接得到小文件内容，如图part-0文件内容所示： 要从HAR读取一个小文件的话，需要用distcp方式，原理也是mapreduce, 指定har路径和输出路径，命令如下：hadoop distcp har:///user/archive/test.har/file-1 /tmp/archive/output HAR总体比较简单，它有什么缺点呢? 1.archive文件一旦创建不可修改即不能append，如果其中某个小文件有问题，得解压处理完异常文件后重新生成新的archive文件; 2.对小文件归档后，原文件并未删除，需要手工删除; 3.创建HAR和解压HAR依赖MapReduce，查询文件时耗很高; 4.归档文件不支持压缩。 SequenceFile存储方案SequenceFile本质上是一种二进制文件格式，类似key-value存储，通过map/reducer的input/output format方式生成。文件内容由Header、Record/Block、SYNC标记组成，根据压缩的方式不同，组织结构也不同，主要分为Record组织模式和Block组织模式。 Record组织模式Record组织模式又包含两种：未压缩状态CompressionType.NONE, 和压缩状态CompressionType.RECORD，未压缩是指不对数据记录进行压缩，压缩态是指对每条记录的value进行压缩，其逻辑结构如下所示： Record结构中包含Record长度、key长度、key值和value值。Sync充斥在Record之间，其作用主要是用于文件位置定位，具体定位方式是：如果提供的文件读取位置不是记录的边界可能在一个Record中间，在实际定位时会定位到所提供位置处之后的第一个Sync边界位置，并从该Sync点往后读相应长度的数据，如果提供的读取位置往后没有Sync边界点，则直接跳转文件末尾；如果提供的文件读取位置是Record边界，则直接从该位置开始读取指定长度的数据。另一种文件定位方式是seek, 这种方式则要求所提供的读取位置是record的边界位置，不然在读取迭代读取下一个位置时会出错。 Block组织模式Block组织模式，其压缩态为CompressionType.BLOCK。与Record模式不同的时，Block是以块为单位进行压缩，即将多条Record写到一个块中，当达到一定大小时，对该块进行压缩，很显然，块的压缩效率会比Record要高很多，避免大量消费IO和CPU等资源。其逻辑结构如下： 从上图中可看出，组织方式变成了块，一个块中又包含了块的记录数，key长度，key值，value长度，value值。每个块之间也有Sync标记，作用同Record方式。 两中模式中，都有header标记，包含了些如版本信息、KEY类名、VALUE类名、是否压缩标记、是否块压缩标记、编码类、元数据信息和Sync标记，其结构如下： SequenceFile示例这里以存储5个小的图片文件为例，演示下如何创建SequenceFile。首先将图片文件上传至hdfs的一个目录。 其次，编写一个MR程序来对上述图片进行转换，将生成的文件存放到/tmp/sequencefile/seq下，MR程序源码在附件SmallFiles.zip中，可自行查看，如下所示： 转换后，会在/tmp/sequencefile/seq目录生成一个part-r-00000文件，这个文件里面就包含了上述5个图片文件的内容，如下所示： 如果要从该SequenceFile中获取所有图片文件，再通过MR程序从文件中将图片文件取出，如下所示： SequenceFile优缺点优点 A.支持基于记录或块的数据压缩; B. 支持splitable,能够作为mr 的输入分片; C. 不用考虑具体存储格式，写入读取较简单. 缺点 A. 需要一个合并文件的过程 B. 依赖于MapReduce C. 二进制文件，合并后不方便查看 CombinedFile存储方案其原理也是基于Map/Reduce将原文件进行转换，通过CombineFileInputFormat类将多个文件分别打包到一个split中，每个mapper处理一个split, 提高并发处理效率，对于有大量小文件的场景，通过这种方式能快速将小文件进行整合。最终的合并文件是将多个小文件内容整合到一个文件中，每一行开始包含每个小文件的完整hdfs路径名，这就会出现一个问题，如果要合并的小文件很多，那么最终合并的文件会包含过多的额外信息，浪费过多的空间，所以这种方案目前相对用得比较少，下面是使用CombineFile的示例： ```hbaseadmin@10-163-161-229:~/program/mr/input&gt; lshbaseadmin@10-163-161-229:~/program/mr/input&gt; touch file-1 file-2 file-3hbaseadmin@10-163-161-229:~/program/mr/input&gt; echo “this is file-1” &gt;file-1hbaseadmin@10-163-161-229:~/program/mr/input&gt; echo “this is file-2” &gt;file-2hbaseadmin@10-163-161-229:~/program/mr/input&gt; echo “this is file-3” &gt;file-3hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop fs -put input /tmp/combinefile/ hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop jar SmallFiles.jar com.fit.dba.mr.util.CombineFileTest /tmp/combinefile/input/ /tmp/combinefile/output hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop fs -ls /tmp/combinefile/outputFound 2 items-rw-r–r– 1 hbaseadmin supergroup 0 2018-03-25 17:26 /tmp/combinefile/output/_SUCCESS-rw-r–r– 1 hbaseadmin supergroup 213 2018-03-25 17:26 /tmp/combinefile/output/part-r-00000hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop fs -ls /tmp/combinefile/output/part-r-00000Found 1 items-rw-r–r– 1 hbaseadmin supergroup 213 2018-03-25 17:26 /tmp/combinefile/output/part-r-00000hbaseadmin@10-163-161-229:~/program/mr&gt; hadoop fs -cat /tmp/combinefile/output/part-r-00000hdfs://10-163-161-229:9000/tmp/combinefile/input/file-1 this is file-1hdfs://10-163-161-229:9000/tmp/combinefile/input/file-2 this is file-2hdfs://10-163-161-229:9000/tmp/combinefile/input/file-3 this is file-3``` 上述用到的转换程序也在附件CombineFileTest.java中。其优点是适用于处理大量比block小的文件和内容比较少的文件合并，尤其是文本类型/sequencefile等文件合并，其缺点是：如果没有合理的设置maxSplitSize，minSizeNode，minSizeRack，则可能会导致一个map任务需要大量访问非本地的Block造成网络开销，反而比正常的非合并方式更慢。 总结上面介绍了三种基于HDFS自身的一些方案，每种方案各有优缺点，其核心思想都是基于map/reduce的方式将多个文件合并成一个文件。在实际使用中，单纯用上述方案还是不太方便，下面简要介绍下目前其它的一些小文件存储方案。 其它小文件存储方案基于HBase的小文件存储方案HBase我们知道主要是key/value存储结构，一个key对应多个列族的多个列值。从2.0版本开始，HBase多了一个MOB的结构，具体参考HBase-11339。具体是什么概念呢，先来看下示意图： 上图是一个关于HBase的架构图，包含HBase的几个组件master、regionserver、hdfs、hfile等。MOB FILE类似StoreFile, 它作为一个单独的对象存储小文件。MOB具体结构如下所示： MOB是由StoreFile和MOB File共同组成。其中，StoreFile存放的数据和HBase正常存储的数据一样，key/value结构，不过value中存储的是关于MOB文件的长度，存放路径等元数据信息，在MOB File中存储的是具体的MOB文件内容，这样通过StoreFile中的key/value可以找到MOB所存放的文件具体位置和大小，最终得到文件内容。 MOB是怎么设置呢，在创建表时我们指定表的MOB属性，如下所示：create &apos;t1&apos;, {NAME =&gt; &apos;f1&apos;, IS_MOB =&gt; true, MOB_THRESHOLD =&gt; 102400} 其中，MOB_THRESHOLD表示MOB对象所能存储的文件对象上限阈值，推荐是存储小于10M的文件。对于MOB的表，我们可以手动触发压缩，有compact_mob和major_compact_mob两种方式。如下所示：compact_mob &apos;t1&apos; compact_mob &apos;t1&apos;,&apos;cf1&apos; major_compact_mob &apos;t1&apos; major_compact_mob &apos;t1&apos;,&apos;cf1&apos; MOB的出现大大提高了我们使用HBase存储小文件的效率，这样无须关注底层HDFS是怎么存储的，只要关注上层逻辑即可，HBase的强大优势也能保证存储的高可靠和稳定性，管理也方便。 基于打包构建索引方案这种方案是目前兄弟部门正在使用的一个小图片存储方案，也是基于HDFS存储实际图片，基于HBase存储元数据信息。这个方案中，主要也是基于压缩的思路，将多个小图上片压缩成一个tar文件存放至HDFS上，通过HBASE记录文件名和HDFS文件的位置映射关系，架构示意图如下： 其具体思路是： 1.业务部门将图片上传至一个中转机，图片按日期目录存储，不同日期上传的图片放到相应日期目录； 2.定期用脚本去将日期目录打包成tar，一天的图片打包成一个以日期命名的tar, tar文件解压后是直接图片文件，即不带日期那层目录，上传至HDFS指定目录； 3.通过tar文件解析程序获取tar文件中各图片文件在tar中的偏移量和长度，这个解析程序最开始是由国外一个程序员Tom Wallroth写的工具,具体地址可以访问:http://github.com/devsnd/tarindexer。 这个工具可以直接在tar文件上解析tar中各文件的偏移量和长度，很方便。 4.得到图片文件在tar的偏移量和长度后，设计HBase rowkey, 将图片名和tar路径设计到rowkey中，并通过在rowkey前缀加盐方式使rowkey随机散列分布在HBase中，避免热点现象；HBase的value存储的是文件的偏移量和长度。这样HBase中就保存了文件的元数据信息； 业务方查询具体某个图片时，根据图片的日期和图片名，先计算出HBase rowkey,再去HBase获取该图片的偏移量和长度；通过偏移量和长度通过HDFS的API去读取HDFS的文件。 其它方案对比下面针对目前行业内用到的其它一些方案作下对比，如下图所示。 总体来说，淘宝的TFS是功能最全的，同时支持大小文件的存储；Ceph也是一种流行的分布式文件存储方案，组内对其调研后感觉比较复杂，不太好管理，不太稳定；FastDFS比较简单，适合存储一些使用场景简单的文件，不太灵活；其它几种没用过，大家可自行上网参阅相关资料。 总结本文介绍了关于HDFS小文件存储的方案，不同方案各具特点，在使用时要根据实际业务场景进行设计，对于既要存储大文件又要存储小文件的场景，我建议在上层作一个逻辑处理层，在存储时先判断是大文件还是小文件，再决定是否用打包压缩还是直接上传至HDFS，可借鉴TFS方案。 参考1 http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html 附件关于文章中用的MR转换程序如下所示：https://github.com/ballwql/common-tool/tree/master/java","link":"/2018/05/15/zh-cn/hadoop-small-file-storage/"},{"title":"【原】浅谈Phoenix在HBase中的应用","text":"业务使用HBase已经有一段时间了，期间也反馈了很多问题，其中反馈最多的是HBase是否支持SQL查询和二级索引，由于HBase在这两块上目前暂不支持，导致业务在使用时无法更好的利用现有的经验来查询HBase。虽然HBase本身不支持SQL，但业界还是有现成的方案来支持，如Hive、Impala、Phoenix等。众多方案各有各的优势，本文主要对Phoenix作一个大概的介绍。 一、介绍Phoenix中文翻译为凤凰, 其最早是Salesforce的一个开源项目，Salesforce背景是一个搞ERP的，ERP软件一个很大的特点就是数据库操作，所以能搞出一个数据库中间件也是很正常的。而后，Phoenix成为Apache基金的顶级项目。 Phoenix具体是什么呢，其本质是用Java写的基于JDBC API操作HBase的开源SQL引擎。它有如下几个功能特性： 我觉得值得关注的几个特性主要有以下几块： 通过JDBC API实现了大部分的java.sql接口，包括元数据API DDL支持：通过CREATE TABLE、DROP TABLE及ALTER TABLE来添加/删除 DML支持：用于逐行插入的UPSERT VALUES,用于相同或不同表之间大量数据传输的UPSERT SELECT,用于删除行的DELETE 事务支持：通过客户端的批处理实现的有限的事务支持(beta测试中) 二级索引支持： 遵循ANSI SQL标准 当前使用Phoenix的公司有很多，如下图所示： 对于我们公司来说，虽然HBase用得多，但用Phoenix的比较少。从自己测试来看，Phoenix确实还存在各种不稳定，如下面描述的几点问题： 最新版本对HBase、Hadoop等有严格版本控制，对于已经用上HBase的业务来说要升级HBase版本适配Phoenix代价太大 与HBase强相关，作为HBase中的一个组件启动，HBase元数据容易遭到破坏 官方提供的创建索引方法，容易导致插入失败，查询失败，程序崩溃等问题 我觉得Phoenix总体思路还是很不错的，但本身太冒进，急于集成新功能，但现有的功能所存在的问题却并未有很好的解决方案，导致版本很多，但没有一个版本能放心在生产环境使用。下面关注一下Phoenix的整体设计思路。 Phoenix架构上面说到，Phoenix是以JDBC驱动方式嵌入到HBase中的，在部署时只有一个包，直接放HBase的lib目录，逻辑构架如下： 从图中可看出，每个RS结点上，都会有一个Phoenix协处理器来处理每个表、每个region的数据，应用端通过Phoneix客户端与HBase客户端打交道，从而实现Sql化访问HBase数据。下面先来说下Coprocessor。 2.1 CoprocessorHBase的协处理器主要受Google BigTable的影响，具体可参考Dean-Keynote-Ladis2009-page 66-67。 对于HBase来说，引入Coprocessor也是为了提供更好的并行计算能力，而无需依赖于Hadoop的MapReduce。同时，基于Coprocessor，可以更好的实现二级索引、复杂过滤规则、权限访问控制等更接地气的特性。Coprocessor有两种类型，Observer和EndPoint。 前者Observer，类似于RDBMS的触发器，主要作用于RegionServer服务端，通过重载Coprocessor框架的Upcall函数插入用户自己的逻辑，这些逻辑只有在固定的事件发生时才会被触发调用执行，主要有三类hook接口：RegionObserver、WALObserver和MasterObserver。RegionObserver提供了一些数据层操作事件的hook,如Put、Get、Delete和Scan等，在每个操作发生或结束时，会触发调用一些前置的Hook(pre+操作,如preGet)或后置的Hook(post+操作,如postGet)；WALObserver提供了WAL相关的Hook；MasterObserver提供了HMaster相关的Hook。 后者EndPoint类似于RDBMS的存储过程，主要作用于客户端，客户端可以调用这些EndPoint执行一段Server端代码，并将Server端代码结果返回给客户端进一步处理，如常见聚合操作，找一张大表某个字段的最大值，如果不用Coprocesser则只能全表扫描，在客户端遍历所有结果找出最大值，且只能利用有限的客户端资源进行迭代计算，无法利用上HBase的并发计算能力；如果用了Coprocessor,则client端可在RegionServer端执行统计每个Region最大值的逻辑，并将Server端结果返回客户端，再找出所有Server端所返回的最大值中的最大值得到最终结果，很明显，这种方式尽量将统计执行下放到Server端，Client端只执行一些最后的聚合，大幅提高了统计效率;还有一个很常见的需求可能就是统计表的行数，其逻辑和上面一样,具体可参考Coprocessor Introduction,在这里就不展开了，后面有机会针对Coprocessor单独展开介绍。 2.2 Phoenix 实现原理Phoenix的SQL实现原理主要也是基于一系列的Scan操作来完成，Scan是HBase的批量扫描过程。这一系列的Scan操作也是分散到各台RegionServer上通过Coprocessor来完成。主要用到的是RegionObserver，通过RegionObserver在postScannerOpen Hook中将RegionScanner替换成支持聚合操作的定制化Scanner，在真正执行聚合时，会通过自定的Scan属性传递给RegionScanner，在这个Scan中也可加入一些过滤规则，尽量减少返回Client的结果。 2.3 Phoenix 数据模型Phoenix在数据模型上是将HBase非关系型形式转换成关系型数据模型 ，如下图所示对于Phoenix来说，HBase的rowkey会被转换成primary key，column family如果不指定则为0否则字段名会带上，qualifier转换成表的字段名，如下是创建一个Phoenix表的例子，以创建表test为例，主键为id即为HBase的rowkey, column family为i, qualifier为name和age。 create table &quot;test&quot; (&quot;id&quot; varchar(20) primary key,&quot;i&quot;.&quot;name&quot; varchar(20) ,&quot;i&quot;.&quot;age&quot; varchar(20)); Phoenix还支持组合primary key，即由多个字段联合组成主键，对于组合主键来说，在HBase底层会把主键的多个字段组合成rowkey显示，其它字段为HBase的qualifier显示。如上面test表，假设id和name为主键，创建表语句又变成: create table &quot;test&quot; (&quot;id&quot; varchar(20), &quot;name&quot; varchar(20) ,&quot;i&quot;.&quot;age&quot; varchar(20),constraint pk PRIMARY KEY(&quot;id&quot;,&quot;name&quot;)); 这样，假设插入一条数据:如下所示 upsert into &quot;test&quot; values (&apos;1&apos;,&apos;a&apos;,&apos;23&apos;); 在HBase中，rowkey即为&quot;1a&quot;, i:age 为 23。这里，可能大家对双引号有点疑问，对于Phoenix来说，加了引号的话，不管是表还是字段名，会变成大小写敏感，不加的话，会统一转换成大写字母。 2.4 Phoenix所支持的语法目前Phoenix已经支持关系型数据库的大部分语法，如下图所示：具体语法用法可参考Phoenix官网,写得比较详细。 三、 Phoenix二级索引我相信，二级索引这个特性应该是大部分用户引入Phoenix主要考虑的因素之一。HBase因其历史原因只支持rowkey索引，当使用rowkey来查询数据时可以很快定位到数据位置。现实中，业务查询需求条件往往比较复杂，带有多个查询字段组合，如果用HBase查的话，只能全表扫描进行过滤，效率很低。而Phoenix支持除rowkey外的其它字段的索引创建，即二级索引，查询效率可大幅提升。 3.1 索引类别3.1.1 Covered Indexes从字面上可理解为覆盖索引，什么意思呢，即索引表中就包含你想要的全部字段数据，这样就只需要通过访问索引表而无需访问主表就能得到数据。创建方式如下： create index my_index on test (v1) include(v2); 当执行select v2 from test where v1=&apos;...&apos;时，就只会查找索引表数据，不会去主表扫描。 3.1.2 Global Indexes全局索引适用于读多写少的场景。全局索引在写数据时会消耗大量资源，所有对数据的增删改操作都会更新索引表，而索引表是分布在各个结点上的，性能会受到影响。好处就是，在读多的场景下如果查询的字段用到索引，效率会很快，因为可以很快定位到数据所在具体结点region上，对于写性能就很慢了，因为每写一次，需要更新所有结点上的索引表数据。创建方式如下： create index my_index on test (v1); 如果执行`select v2 from test where v1=’…’， 实际是用不上索引的，因为v2不在索引字段中，对于全局索引来说，如果查询的字段不包含在索引表中，则还是会去全表扫描主表。 3.1.3 Local Indexes局部索引适用于写多读少场景，和全局索引类似，Phoenix会在查询时自动选择是否使用索引。如果定义为局部索引，索引表数据和主表数据会放在同一regionserver上，避免写操作时跨节点写索引表带来的额外开销(如Global Indexes)。当使用局部索引查询时，即使查询字段不是索引字段，索引表也会正常使用，这和Global Indexes是有区别的。在4.8版本之前，所有局部索引数据存放在一个单独的共享表中，4.8之后是存储在主表的一个独立的列族中。因为是局部索引，所以在client端查询使用索引时，需要扫描每个结点上的索引表以得到数据所在具体region位置，当region多时，查询时耗会很高，所以查询性能比较低，适合读少写多场景。创建局部索引方式： create local index my_index on test (v1); 3.2 Mutable Indexing 和Immutable Indexing3.2.1 IMMutable Indexing不可变索引主要创建在不可变表上，适用于数据只写一次不会有Update等操作，在什么场景下会用到不可变索引呢，很经典的时序数据:write once read many times。在这种场景下，所有索引数据（primary和index)要么全部写成功，要么一个失败全都失败返回错误给客户端。不可变索引用到场景比较少，下面是创建不可变索引的方式： create table test (pk VARCHAR primary key,v1 VARCHAR, v2 VARCHAR) IMMUTABLE_ROWS=true; 即在创建表时指定IMMUTABLE_ROWS参数为true，默认这个参数为false。如果想把不可变索引改为可变索引，可用alter修改： alter table test set IMMUTABLE_ROWS=false; 3.2.2 Mutable Indexing可变索引意思是在修改数据如Insert、Update或Delete数据时会同时更新索引。这里的索引更新涉及WAL，即主表数据更新时，会把索引数据也同步更新到WAL，只有当WAL同步到磁盘时才会去更新实际的primary/index数据，以保证当中间任何一个环节异常时可通过WAL来恢复主表和索引表数据。 四、性能在官网，有作一个性能测试，主要是将Phoenix和Hive、Impala作一个对比。先来看下和Hive的性能对比，测试基准如下： select count(1) from table over 10M and 100M rows. Data is 5 narrow columns. Number of Region Servers: 4 (HBase heap: 10GB, Processor: 6 cores @ 3.3GHz Xeon) 测试结果：从图中可看出，带有Key过滤的Phoenix耗时最少，不带Key过滤的Phoenix和基于HDFS的Hive性能差不多，直接基于HBase的Hive性能最差。 再来看下和Impala的对比，测试基准如下： select count(1) from table over 1M and 5M rows. Data is 3 narrow columns. Number of Region Server: 1 (Virtual Machine, HBase heap: 2GB, Processor: 2 cores @ 3.3GHz Xeon) 测试结果：从图中可看出，Impala执行时间比Phoenix长很多，原因大概有几点：Impala基于内存进行并行计算，容易内存吃紧，对HBase和HDFS的支持也还远远不够，性能比较差。 我在自己的HBase测试集群也作了下测试，主要测试数据插入和一些SQL操作的查询时耗。测试集群如下： 先来测试下插入100万记录的测试基准，如下所示： 1.创建基本表，表主键由4个字段组成，HOST字段称为First PK,DOMAIN为Second PK, 依此类推，SPLIT ON指定8个分区。 CREATE TABLE IF NOT EXISTS %s (HOST CHAR(2) NOT NULL, DOMAIN VARCHAR NOT NULL, FEATURE VARCHAR NOT NULL, DATE DATE NOT NULL, USAGE.CORE BIGINT, USAGE.DB BIGINT, STATS.ACTIVE_VISITOR INTEGER CONSTRAINT PK PRIMARY KEY (HOST, DOMAIN, FEATURE, DATE)) SPLIT ON (&apos;CSGoogle&apos;,&apos;CSSalesforce&apos;,&apos;EUApple&apos;,&apos;EUGoogle&apos;,&apos;EUSalesforce&apos;,&apos;NAApple&apos;,&apos;NAGoogle&apos;,&apos;NASalesforce&apos;) 2.插入100万行记录 3.执行如下查询条件测试 Query # 1 - Count - SELECT COUNT(1) FROM PERFORMANCE_1000000; Query # 2 - Group By First PK - SELECT HOST FROM PERFORMANCE_1000000 GROUP BY HOST; Query # 3 - Group By Second PK - SELECT DOMAIN FROM PERFORMANCE_1000000 GROUP BY DOMAIN; Query # 4 - Truncate + Group By - SELECT TRUNC(DATE,&apos;DAY&apos;) DAY FROM PERFORMANCE_1000000 GROUP BY TRUNC(DATE,&apos;DAY&apos;); Query # 5 - Filter + Count - SELECT COUNT(1) FROM PERFORMANCE_1000000 WHERE CORE&lt;10; 测试结果如下： 插入100万条记录耗时70s Query #1 耗时1.032s Query #2 耗时0.025s Query #3 耗时0.615s Query #4 耗时0.608s Query #5 耗时1.026s 具体结果如下： csv columns from database. CSV Upsert complete. 1000000 rows upserted Time: 69.672 sec(s) COUNT(1) ---------------------------------------- 1000000 Time: 1.032 sec(s) HO -- CS EU NA Time: 0.025 sec(s) DOMAIN ---------------------------------------- Apple.com Google.com Salesforce.com Time: 0.615 sec(s) DAY ----------------------- 2018-01-28 00:00:00.000 2018-01-29 00:00:00.000 2018-01-30 00:00:00.000 2018-01-31 00:00:00.000 2018-02-01 00:00:00.000 2018-02-02 00:00:00.000 2018-02-03 00:00:00.000 2018-02-04 00:00:00.000 2018-02-05 00:00:00.000 2018-02-06 00:00:00.000 2018-02-07 00:00:00.000 2018-02-08 00:00:00.000 2018-02-09 00:00:00.000 Time: 0.608 sec(s) COUNT(1) ---------------------------------------- 20209 Time: 1.026 sec(s) 还作了下三种不同数量级下的性能对比，作了5种SQL查询操作对比，如上测试基准第3条所描述的查询条件，结果如下： 从结果看，随着数量级的增加，查询时耗也随之增加，有一个例外，就是当用First PK索引字段作聚合查询时，用时相差不大。总的来说，Phoenix在用到索引时查询性能会比较好。那对于Count来说，如果不用Phoenix,用HBase自带的Count耗时是怎样的呢，测了一下，HBase Count 100万需要33s, 500万需要139s,1000万需要284s，性能还是很差的。对于大表来说基本不能用Count来统计行数，还得依赖于基于Coprocessor机制来统计。 从上面测试来看下，Phoenix的性能不能说最好，也存在各种问题，就如开篇说的，版本不稳定，BUG过多，容易影响集群稳定性。 五、总结总的来说，目前并没有一种很完美的方案来解决SQL查询、二级索引问题，都或多或少存在各种问题。不过HBase的Coprocessor是个好东西，很多功能可以基于此特性进行二次开发，后续可以深入研究一下。 六、参考[1] https://community.hortonworks.com/articles/61705/art-of-phoenix-secondary-indexes.html [2] https://github.com/forcedotcom/phoenix/wiki/Secondary-Indexing [3] http://phoenix.apache.org/secondary_indexing.html","link":"/2017/11/11/zh-cn/phoenix-introduction/"}],"tags":[{"name":"HBase","slug":"HBase","link":"/zh-cn/tags/HBase/"},{"name":"python","slug":"python","link":"/zh-cn/tags/python/"},{"name":"Hadoop","slug":"Hadoop","link":"/zh-cn/tags/Hadoop/"}],"categories":[{"name":"HBase","slug":"HBase","link":"/zh-cn/categories/HBase/"},{"name":"Python","slug":"Python","link":"/zh-cn/categories/Python/"},{"name":"Hadoop","slug":"Hadoop","link":"/zh-cn/categories/Hadoop/"}]}