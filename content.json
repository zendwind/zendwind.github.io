{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"","text":"欢迎访问博客","link":"/other/announce.html"},{"title":"捐赠墙","text":"","link":"/donate/index.html"},{"title":"About Me","text":"Welcome to my blog.It’s ballen, a cloud compute solution engineer in Tencent. I am interested in various database products, such as MySQL, HBase, Redis, MongoDB, DynamoDB, CosmosDB, Tencent TcaplusDB, etc. I like recording whatever I see and think. Four years ago, I started engaging in DBA, and aimed at solving various problems of HBase/Hadoop. Meanwhile, learned how to be a competent DevOps engineer. Two years ago, I focused on developing fincinal backend system, and learned how to desgin a data-intensive application with micro service framework. It’s a great opportunity for me to connect with the wonderful Tech World. Hope your guys can enjoy it and feel free to contact me if you have any questions. Nickname: ballenEmail: zendwind@gmail.comGithub: @zendwind","link":"/about/index.html"}],"posts":[{"title":"An Introduction to HBase Native API","text":"HBase is a distributed database, this article will introduce HBase Native API to help everyone have a preliminary understanding of HBase API. HBase intruductionHBase is a distributed database , which can support random/sequential write/read of batch data.HBase architecture is shown in the following figure. HBase Master is the key component, in charge of the whole cluster management, such as node heartbeat, meta data, load balance, fault tolerance, and region management. Client is in charge of the communication between application and hbase. Zookeeper is a distributed coordination component, in charge of master selection and cluster status management. Regionserver is the node of cluster, in charge of data storage, region management, write/read request response, .e.g. HBase data model is shown in the following table. As we can know from above figure, the model includes four pieces, rowkey, timestamp, column family and qualifer. Rowkey is the unique key of hbase, each record in hbase has its own timestamp to identify the time of insert or update; CF(Column Family) consists of serveral similar columns(qualifiers), different attributes of column distribute different cf in order to manage more conveniently. HBase Shell APIHow to get row data from hbase? HBase provides two kinds of api to complete it, scan and get. The GET method aims to get one row by rowkey each time, nevertheless the SCAN method aims to get multi rows by rowkey-prefix each time. Next, we will introduce GET and SCAN method. First, Get is the standard api of hbase. If we want to get r1 rowkey data by rowkey, the command in hbase shell will like this: get &apos;table&apos;,&apos;r1&apos; get &apos;table&apos;, &apos;r1&apos;, &apos;cf:a&apos; The first command will return all columns data (cf:a, cf:b), and the second command will only return &apos;cf:a&apos; data. Second, Scan is also the standard api of hbase. If we want to get both &apos;r1&apos; and &apos;r2&apos; data, and hypothesis both of them have common rowkey-prefix,then we can use scan like this: scan &apos;table&apos;,{STARTROW=&gt;&apos;row-start-prefix&apos;,ENDROW=&gt;&apos;row-end-prefix&apos;} As we can see this command, we must specify two parameters STARTROW and ENDROW, which symbolize the rowkey data range of scan in hbase, if we only need specified number of lines, the other parameter can help us to do this, like this: scan &apos;table&apos;,{STARTROW=&gt;&apos;row-start-prefix&apos;, ENDROW=&gt;&apos;row-end-prefix&apos;,LIMIT=&gt;2} The LIMIT instruction tells hbase to scan the limit rows and then return result. If we need return specified value of row, we can use filter to help us, like this: scan &apos;table&apos;,{STARTROW=&gt;&apos;row-start-prefix&apos;,ENDROW=&gt;&apos;row-end-prefix&apos;,FILTER=&gt;&quot;SingleColumnValueFilter(&apos;cf&apos;,&apos;a&apos;,=,&apos;binary:1&apos;)&quot;} This command will return all rows of &apos;cf:a&apos; equals 1 . There are also many other filter method in hbase, such PrefixFilter, CompareFilter and so on. In future articles , I will introduce more about filters knowledge. ConclusionIn this paper, I only introduce the simple hbase shell api GET and SCAN. In next paper, I will start to introduce more about java api to get hbase data.","link":"/2018/10/22/hbase-api/"},{"title":"How to Deploy Airflow with Docker","text":"This post introduces how to deploy Airflow with Docker. Prerequisites CentOS7 (64-bit) TencentCloud CVM (4C,8G) Python3.6.0Docker InstallDocker Installyum install -y docker #in centos8, yum install -y docker-ce systemctl start docker Docker Compose Install # download from github with curl curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose #install python module pip install docker-compose #check docker-compose version docker-compose version Airflow Docker Install Pull from Repositorydocker pull puckel/docker-airflow Download Git Packagegit clone https://github.com/puckel/docker-airflow.git Airflow LocalExecutor DeployIntroductionThe Airflow supports various deployment mode, such as SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor. Airflow Local Executor ModeRunning Airflow on a LocalExecutor exemplifies single-node architecture. In dev or test environment, this mode is simple and easy for users to use Airflow. Airflow CeleryExecutor ModeRunning Airflow on a CeleryExecutor exemplifies distributed architecture, and communicates with Celery. For production environment, this mode is popular for users to deploy. In this solution, only introduce how to deploy a LocalExecutor with docker. LocalExecutor Deploy Step 1. Modify airflow.cfgGo into docker-airflow/config directory, open airflow.cfg and modify two items as below:```#specify executor typeexecutor = LocalExecutor #specify postgresql data sourcesql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow * Step2. Modify Dockerfile The dockerfile in puckel image is based on Debian and includes all of steps how to build the airflow docker image. So it&apos;s unnecessary for you to rebuild the whole image, the simplified dockerfile is as below, which only contains basic steps. FROM puckel/docker-airflow:latest #RUN pip install –user psycopg2-binaryARG AIRFLOW_USER_HOME=/usr/local/airflowUSER root #RUN pip install xxxENV AIRFLOW_HOME=/usr/local/airflowCOPY config/airflow.cfg /usr/local/airflow/airflow.cfg RUN chown -R airflow: ${AIRFLOW_USER_HOME} EXPOSE 8080 5555 8793 USER airflowWORKDIR ${AIRFLOW_USER_HOME}ENTRYPOINT [“/entrypoint.sh”]CMD [“webserver”] * Step3. Modify docker-compose-LocalExecutor.yml This file includes the containers we need to deploy, such as postgres and webserver. Meanwhile, specify the `volumes` for container. version: ‘3.7’services: postgres: image: postgres:9.6 environment: - POSTGRES_USER=airflow - POSTGRES_PASSWORD=airflow - POSTGRES_DB=airflow ports: - &quot;5432:5432&quot; volumes: - ./data/postgres:/var/lib/postgresql/data logging: options: max-size: 10m max-file: &quot;3&quot; webserver: image: puckel/docker-airflow:latest restart: always depends_on: - postgres environment: - LOAD_EX=n - EXECUTOR=Local logging: options: max-size: 10m max-file: &quot;3&quot; volumes: - ./dags:/usr/local/airflow/dags # - ./plugins:/usr/local/airflow/plugins ports: - &quot;8080:8080&quot; command: webserver healthcheck: test: [&quot;CMD-SHELL&quot;, &quot;[ -f /usr/local/airflow/airflow-webserver.pid ]&quot;] interval: 30s timeout: 30s retries: 3 __Note__: the image version of webserver need change to `latest` version * Step4. Rebuild image cd puckel-docker-airflowdocker build –rm -t puckel/docker-airflow . * Step5. Deploy container cd puckel-docker-airflowdocker-compose -f docker-compose-LocalExecutor.yml up -d Then you can execute `docker ps` to get the container list. If you want to restart container, such as webserver, the command is as below: docker-compose -f ./docker-compose-LocalExecutor.yml stop webserverdocker-compose -f ./docker-compose-LocalExecutor.yml start webserver * Step6. Deploy Dag Add a dag file (Python) in `dags` directory which is defined as volumes of `webserver` container in docker-compose-LocalExecutor.yml. Then, you can find it on the Airflow website. http://localhost:8080 # FAQ 1. `airflow initdb` error: airflow.exceptions.AirflowException: Could not create Fernet object: Incorrect padding python -c “from cryptography.fernet import Fernet;print(Fernet.generate_key().decode())”3vWG_GoK_2MtU1fqRwCKrfNVsKW-ZcpdP8Ltz51xm64= export AIRFLOWCOREFERNET_KEY=3vWG_GoK_2MtU1fqRwCKrfNVsKW-ZcpdP8Ltz51xm64=```","link":"/2020/04/04/airflow-docker-install/"},{"title":"How to Migrate DynamoDB to Another Platform","text":"This post aims to describe how to migrate DyanmoDB data to other Platform. The whole post will include three subtopics as below: Part One: Export DynamoDB Data to S3 with AWS Data Pipeline Part Two: Import S3 Data to Tencent TcaplusDB Part Three: Principles of Design A Migration System Why Migrate?Different customers have different cases. As for DynamoDB, it is a popular NoSQL product around the world, and holds most of the market share. So why need to migrate data from DynamoDB to other Platforms. I think the leading causes are as below: PriceAs we known, DynamoDB has two pricing mode: on-demand and provisioned. Both of them are cost-expensive, if your business has large traffic in read/write, the DynamoDB will occupy a large proportion. IntegrationThe DynamoDB is strongly bound with AWS, which means you must use AWS other products to work with DynamoDB. I think it is not a good news for most of customers who are unwilling to be tied on one boat. Export ModeThere are three approaches for exporting DynamoDB data: Export to .csv from DynamoDB PortalThis mode is the most easiest way to export data, however it is just available for those small tables. If table has massive data, this way will be heavy workload, because only 100 items can be exported each time. Export with ScanThis mode is the most popular way to export all of DynamoDB data, however, it is cost-expensive and will exhaust a large number of RCU. Meanwhile, if you use Scan to export data , you must consider how to avoid impacting online business, such as traffic control, bandwidth control, maybe the third-party tool need to be introduced to limit the migrating rate (Google Guava RateLimit). Export with Data PipelineThis mode is the most cost-effective way to export all of DynamoDB data. It uses AWS EMR to process off-line data parallelly, and its core concept is Map/Reduce. With this offline mode, the scope of business impact is small. In next post, we will use this approach to finish the migrating work. Migrate Issues Data ChangeThe business produces massive real-time data, how to solve the conflict problem in new platform. I think the Incremental Migration is essential and must be built before Full Migration. In accordance with the characteristics of data change, data streaming pipeline is important for capturing the changed data (e.g, update, delete).","link":"/2020/03/28/dynamodb-migrate-s3/"},{"title":"How to Deploy Vuepress with Serverless Framework","text":"Serverless Framework is very popular all over the world, and has 30,000+ stars in Github. It is friendly for many cloud platforms, such as AWS, GCP, Azure, TencentCloud, AliCloud,etc.This post will present how to deploy vuepress (a simple static site generator) with Serverless Framework on TencentCloud. PrerequisitesNodeJS and NPM InstallThe installation can refer to How to Install Node.js and NPM on a Mac. Serverless Installnpm install -g serverless Vuepress Installnpm install -g vuepress COS Bucket Create Apply for TencentCloud account Check out the COS documentation page for an example of creating a COS bucket in relevant region. IntroductionServerless FrameworkThe introduction about serverless can refer to official website. VuepressVuepress includes two parts: one is minimalistic static site generator, the other is theme. The detail informations can refer to official website. Best PracticeOperation StepsStep1.Project Preparationmkdir -p vuepress_blog/docs cd vuepress_blog touch package.json touch docs/README.md echo &quot;Hello, My first Vuepress Blog&quot; &gt;&gt;README.md The package.json is as below:{ &quot;scripts&quot;: { &quot;docs:dev&quot;: &quot;vuepress dev docs&quot;, &quot;docs:build&quot;: &quot;vuepress build docs&quot;, &quot;docs:deploy&quot;: &quot;cd docs/.vuepress &amp;&amp; sls&quot; } } docs:dev : start a dev environment docs:build : build a static site docs:deploy : deploy the static site to cos with Serverless Framework Note: With package.json, you can integrate your project with CI service easily. The final directory structure is as follows:. |-- docs | |--.vuepress | | |--dist | | | |--index.html | | | |--404.html | | | |--assets | | | | |--css | | | | |--js | | | | |--img | |--README.md |-- package.json Step2. Run Dev Modenpm run docs:dev # or vuepress dev docs/ This command will start a dev environment, and you can browse the static site with localhost:8080 Step3. Build Static Sitenpm run docs:build This command will generate static files in vuepress_blog/docs/.vuepress/dist, such as: drwxr-xr-x 5 xxx staff 160B Apr 23 11:29 assets -rw-r--r-- 1 xxx staff 1.2K Apr 23 11:29 404.html -rw-r--r-- 1 xxx staff 2.1K Apr 23 11:29 index.html Step4. Config serverless.ymlcd vuepress_blog/docs/.vuepress touch serverless.yml The serverless.yml is as below:name: myblog myblog: component: &quot;@serverless/tencent-website&quot; inputs: code: src: ./dist # Upload static files index: index.html error: 404.html region: ap-hongkong bucketName: your cos bucket nameNote: The component @serverless/tencent-website is suitable for tencent cloud, you can see details in github project Step5. Serverless DeployConfig the .env file for authorization of TencentCloudcd vuepress_blog/docs/.vuepress vim .env # replace your secret_id and secret_key of tencent cloud TENCENT_APP_ID=xxx # your account id TENCENT_SECRET_ID=xxx # secret id TENCENT_SECRET_KEY=xxx # secret key TENCENT_TOKEN=xxx # access token Then execute sls to deploy static site to COS:npm run docs:deploy Meanwhile, we can integrate this command in config.js as below: Step6. Check DeploymentOpen the COS http url on browser to see the index.html. ExtensionIf you want to build a documentation system, the Vuepress will be your choice. The detail description can refer to In-Depth VuePress Tutorial: Vue-Powered Docs &amp; Blog. Add a config fileAdd a config.js in vuepress_blog/docs/.vuepress directory, for example: module.exports = { title: &apos;Welcome to TcaplusDB World&apos;, description: &apos;Just playing around&apos;, themeConfig: { nav: [ { text: &apos;Home&apos;, link: &apos;/&apos; }, { text: &apos;Blog&apos;, link: &apos;/blog/&apos; }, { text: &apos;External&apos;, link: &apos;https://cloud.tencent.com/product/tcaplusdb&apos; }, ], sidebar: [ &apos;/&apos;, &apos;/blog/&apos; ] } } Add SubdirectoryCreate blog subdirectory in vuepress_blog/docs, then touch a README.md and first-post.md in blog directory. Config Vue SupportVuepress supports flexible vue component config. Take blog for example.First, add component directory in vuepress_blog/docs/.vuepress cd vuepress_blog/docs/.vuepress mkdir component cd component touch BlogIndex.vue The BlogIndex.vue is as follows: &lt;template&gt; &lt;div&gt; &lt;div v-for=&quot;post in posts&quot;&gt; &lt;h2&gt; &lt;router-link :to=&quot;post.path&quot;&gt;{{ post.frontmatter.title }}&lt;/router-link&gt; &lt;/h2&gt; &lt;p&gt;{{ post.frontmatter.description }}&lt;/p&gt; &lt;p&gt;&lt;router-link :to=&quot;post.path&quot;&gt;Read more&lt;/router-link&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; export default { computed: { posts() { return this.$site.pages .filter(x =&gt; x.path.startsWith(&apos;/blog/&apos;) &amp;&amp; !x.frontmatter.blog_index) .sort((a, b) =&gt; new Date(b.frontmatter.date) - new Date(a.frontmatter.date)); } } } &lt;/script&gt; Second, config README.md in blog directory --- blog_index: true --- # Blog Welcome to TcaplusDB blog &lt;BlogIndex /&gt; The blog_index is used to make sure that the blog index isn’t listed in the posts. When filtering the posts in the posts computed property of the BlogIndex component. The final directory structure is as below: . ├── docs │ ├── .vuepress │ │ ├── .env │ │ ├── components │ │ │ └── BlogIndex.vue │ │ ├── config.js │ │ └── serverless.yml │ ├── README.md │ ├── blog │ │ ├── README.md │ │ └── first-post.md │ └── guide │ └── README.md ├── package-lock.json └── package.json Third, rebuild and deploy static site npm run docs:build npm run docs:deploy The demo is shown as below picture: SummaryThis post introduces how to deploy a Vuepress static site with Serverless Framework to COS of TencentCloud. It is convient for users to have their simple documentation system quickly.","link":"/2020/04/23/serverless-deploy-vuepress/"}],"tags":[{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"Airflow","slug":"Airflow","link":"/tags/Airflow/"},{"name":"DynamoDB,COS","slug":"DynamoDB-COS","link":"/tags/DynamoDB-COS/"},{"name":"vuepress,serverless","slug":"vuepress-serverless","link":"/tags/vuepress-serverless/"}],"categories":[{"name":"HBase","slug":"HBase","link":"/categories/HBase/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"TencentCloud","slug":"TencentCloud","link":"/categories/TencentCloud/"},{"name":"Serverless","slug":"Serverless","link":"/categories/Serverless/"}]}