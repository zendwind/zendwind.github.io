{"pages":[{"title":"About Me","text":"Welcome to my blog.It’s ballen, a cloud compute solution engineer in Tencent. I am interested in various database products, such as MySQL, HBase, Redis, MongoDB, DynamoDB, CosmosDB, Tencent TcaplusDB, etc. I like recording whatever I see and think. Four years ago, I started engaging in DBA, and aimed at solving various problems of HBase/Hadoop. Meanwhile, learned how to be a competent DevOps engineer. Two years ago, I focused on developing fincinal backend system, and learned how to desgin a data-intensive application with micro service framework. It’s a great opportunity for me to connect with the wonderful Tech World. Hope your guys can enjoy it and feel free to contact me if you have any questions. Nickname: ballenEmail: zendwind@gmail.comGithub: @zendwind","link":"/about/index.html"}],"posts":[{"title":"How to Deploy Airflow with Docker","text":"This post introduces how to deploy Airflow with Docker. Prerequisites CentOS7 (64-bit) TencentCloud CVM (4C,8G) Python3.6.0Docker InstallDocker Installyum install -y docker #in centos8, yum install -y docker-ce systemctl start docker Docker Compose Install # download from github with curl curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose #install python module pip install docker-compose #check docker-compose version docker-compose version Airflow Docker Install Pull from Repositorydocker pull puckel/docker-airflow Download Git Packagegit clone https://github.com/puckel/docker-airflow.git Airflow LocalExecutor DeployIntroductionThe Airflow supports various deployment mode, such as SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor. Airflow Local Executor ModeRunning Airflow on a LocalExecutor exemplifies single-node architecture. In dev or test environment, this mode is simple and easy for users to use Airflow. Airflow CeleryExecutor ModeRunning Airflow on a CeleryExecutor exemplifies distributed architecture, and communicates with Celery. For production environment, this mode is popular for users to deploy. In this solution, only introduce how to deploy a LocalExecutor with docker. LocalExecutor Deploy Step 1. Modify airflow.cfgGo into docker-airflow/config directory, open airflow.cfg and modify two items as below:```#specify executor typeexecutor = LocalExecutor #specify postgresql data sourcesql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow * Step2. Modify Dockerfile The dockerfile in puckel image is based on Debian and includes all of steps how to build the airflow docker image. So it&apos;s unnecessary for you to rebuild the whole image, the simplified dockerfile is as below, which only contains basic steps. FROM puckel/docker-airflow:latest #RUN pip install –user psycopg2-binaryARG AIRFLOW_USER_HOME=/usr/local/airflowUSER root #RUN pip install xxxENV AIRFLOW_HOME=/usr/local/airflowCOPY config/airflow.cfg /usr/local/airflow/airflow.cfg RUN chown -R airflow: ${AIRFLOW_USER_HOME} EXPOSE 8080 5555 8793 USER airflowWORKDIR ${AIRFLOW_USER_HOME}ENTRYPOINT [“/entrypoint.sh”]CMD [“webserver”] * Step3. Modify docker-compose-LocalExecutor.yml This file includes the containers we need to deploy, such as postgres and webserver. Meanwhile, specify the `volumes` for container. version: ‘3.7’services: postgres: image: postgres:9.6 environment: - POSTGRES_USER=airflow - POSTGRES_PASSWORD=airflow - POSTGRES_DB=airflow ports: - &quot;5432:5432&quot; volumes: - ./data/postgres:/var/lib/postgresql/data logging: options: max-size: 10m max-file: &quot;3&quot; webserver: image: puckel/docker-airflow:latest restart: always depends_on: - postgres environment: - LOAD_EX=n - EXECUTOR=Local logging: options: max-size: 10m max-file: &quot;3&quot; volumes: - ./dags:/usr/local/airflow/dags # - ./plugins:/usr/local/airflow/plugins ports: - &quot;8080:8080&quot; command: webserver healthcheck: test: [&quot;CMD-SHELL&quot;, &quot;[ -f /usr/local/airflow/airflow-webserver.pid ]&quot;] interval: 30s timeout: 30s retries: 3 __Note__: the image version of webserver need change to `latest` version * Step4. Rebuild image cd puckel-docker-airflowdocker build –rm -t puckel/docker-airflow . * Step5. Deploy container cd puckel-docker-airflowdocker-compose -f docker-compose-LocalExecutor.yml up -d Then you can execute `docker ps` to get the container list. If you want to restart container, such as webserver, the command is as below: docker-compose -f ./docker-compose-LocalExecutor.yml stop webserverdocker-compose -f ./docker-compose-LocalExecutor.yml start webserver * Step6. Deploy Dag Add a dag file (Python) in `dags` directory which is defined as volumes of `webserver` container in docker-compose-LocalExecutor.yml. Then, you can find it on the Airflow website. http://localhost:8080 # FAQ 1. `airflow initdb` error: airflow.exceptions.AirflowException: Could not create Fernet object: Incorrect padding python -c “from cryptography.fernet import Fernet;print(Fernet.generate_key().decode())”3vWG_GoK_2MtU1fqRwCKrfNVsKW-ZcpdP8Ltz51xm64= export AIRFLOWCOREFERNET_KEY=3vWG_GoK_2MtU1fqRwCKrfNVsKW-ZcpdP8Ltz51xm64=```","link":"/2020/04/04/airflow-docker-install/"},{"title":"How to Migrate DynamoDB to Another Platforms","text":"This post aims to describe how to migrate DyanmoDB data to other Platform. The whole post will include four subtopics as below: Part One: Export DynamoDB Data to S3 with AWS Data Pipeline Part Two: Import S3 Data to Tencent TcaplusDB Part Three: Principles of Design A Migration System Part Four: DynamoDB Real-time Migration Solution Why Migrate?Different customers have different cases. As for DynamoDB, it is a popular NoSQL product around the world, and holds most of the market share. So why need to migrate data from DynamoDB to other Platforms. I think the leading causes are as below: PriceAs we known, DynamoDB has two pricing mode: on-demand and provisioned. Both of them are cost-expensive, if your business has large traffic in read/write, the DynamoDB will occupy a large proportion. IntegrationThe DynamoDB is strongly bound with AWS, which means you must use AWS other products to work with DynamoDB. I think it is not a good news for most of customers who are unwilling to be tied on one boat. Export ModeThere are three approaches for exporting DynamoDB data: Export to .csv from DynamoDB PortalThis mode is the most easiest way to export data, however it is just available for those small tables. If table has massive data, this way will be heavy workload, because only 100 items can be exported each time. Export with ScanThis mode is the most popular way to export all of DynamoDB data, however, it is cost-expensive and will exhaust a large number of RCU. Meanwhile, if you use Scan to export data , you must consider how to avoid impacting online business, such as traffic control, bandwidth control, maybe the third-party tool need to be introduced to limit the migrating rate (Google Guava RateLimit). Export with Data PipelineThis mode is the most cost-effective way to export all of DynamoDB data. It uses AWS EMR to process off-line data parallelly, and its core concept is Map/Reduce. With this offline mode, the scope of business impact is small. In next post, we will use this approach to finish the migrating work. Get Started with Migration Part One: Export DynamoDB Data to S3 with AWS Data Pipeline Part Two: Import S3 Data to Tencent TcaplusDB Part Three: Principles of Design A Migration System Part Four: DynamoDB Real-time Migration Solution","link":"/2020/03/28/dynamodb-migrate-s3/"},{"title":"An Introduction to HBase Native API","text":"HBase is a distributed database, this article will introduce HBase Native API to help everyone have a preliminary understanding of HBase API. HBase intruductionHBase is a distributed database , which can support random/sequential write/read of batch data.HBase architecture is shown in the following figure. HBase Master is the key component, in charge of the whole cluster management, such as node heartbeat, meta data, load balance, fault tolerance, and region management. Client is in charge of the communication between application and hbase. Zookeeper is a distributed coordination component, in charge of master selection and cluster status management. Regionserver is the node of cluster, in charge of data storage, region management, write/read request response, .e.g. HBase data model is shown in the following table. As we can know from above figure, the model includes four pieces, rowkey, timestamp, column family and qualifer. Rowkey is the unique key of hbase, each record in hbase has its own timestamp to identify the time of insert or update; CF(Column Family) consists of serveral similar columns(qualifiers), different attributes of column distribute different cf in order to manage more conveniently. HBase Shell APIHow to get row data from hbase? HBase provides two kinds of api to complete it, scan and get. The GET method aims to get one row by rowkey each time, nevertheless the SCAN method aims to get multi rows by rowkey-prefix each time. Next, we will introduce GET and SCAN method. First, Get is the standard api of hbase. If we want to get r1 rowkey data by rowkey, the command in hbase shell will like this: get &apos;table&apos;,&apos;r1&apos; get &apos;table&apos;, &apos;r1&apos;, &apos;cf:a&apos; The first command will return all columns data (cf:a, cf:b), and the second command will only return &apos;cf:a&apos; data. Second, Scan is also the standard api of hbase. If we want to get both &apos;r1&apos; and &apos;r2&apos; data, and hypothesis both of them have common rowkey-prefix,then we can use scan like this: scan &apos;table&apos;,{STARTROW=&gt;&apos;row-start-prefix&apos;,ENDROW=&gt;&apos;row-end-prefix&apos;} As we can see this command, we must specify two parameters STARTROW and ENDROW, which symbolize the rowkey data range of scan in hbase, if we only need specified number of lines, the other parameter can help us to do this, like this: scan &apos;table&apos;,{STARTROW=&gt;&apos;row-start-prefix&apos;, ENDROW=&gt;&apos;row-end-prefix&apos;,LIMIT=&gt;2} The LIMIT instruction tells hbase to scan the limit rows and then return result. If we need return specified value of row, we can use filter to help us, like this: scan &apos;table&apos;,{STARTROW=&gt;&apos;row-start-prefix&apos;,ENDROW=&gt;&apos;row-end-prefix&apos;,FILTER=&gt;&quot;SingleColumnValueFilter(&apos;cf&apos;,&apos;a&apos;,=,&apos;binary:1&apos;)&quot;} This command will return all rows of &apos;cf:a&apos; equals 1 . There are also many other filter method in hbase, such PrefixFilter, CompareFilter and so on. In future articles , I will introduce more about filters knowledge. ConclusionIn this paper, I only introduce the simple hbase shell api GET and SCAN. In next paper, I will start to introduce more about java api to get hbase data.","link":"/2018/10/22/hbase-api/"},{"title":"How to Deploy Vuepress with Serverless Framework","text":"Serverless Framework is very popular all over the world, and has 30,000+ stars in Github. It is friendly for many cloud platforms, such as AWS, GCP, Azure, TencentCloud, AliCloud,etc.This post will present how to deploy vuepress (a simple static site generator) with Serverless Framework on TencentCloud. PrerequisitesNodeJS and NPM InstallThe installation can refer to How to Install Node.js and NPM on a Mac. Serverless Installnpm install -g serverless Vuepress Installnpm install -g vuepress COS Bucket Create Apply for TencentCloud account Check out the COS documentation page for an example of creating a COS bucket in relevant region. IntroductionServerless FrameworkThe introduction about serverless can refer to official website. VuepressVuepress includes two parts: one is minimalistic static site generator, the other is theme. The detail informations can refer to official website. Best PracticeOperation StepsStep1.Project Preparationmkdir -p vuepress_blog/docs cd vuepress_blog touch package.json touch docs/README.md echo &quot;Hello, My first Vuepress Blog&quot; &gt;&gt;README.md The package.json is as below:{ &quot;scripts&quot;: { &quot;docs:dev&quot;: &quot;vuepress dev docs&quot;, &quot;docs:build&quot;: &quot;vuepress build docs&quot;, &quot;docs:deploy&quot;: &quot;cd docs/.vuepress &amp;&amp; sls&quot; } } docs:dev : start a dev environment docs:build : build a static site docs:deploy : deploy the static site to cos with Serverless Framework Note: With package.json, you can integrate your project with CI service easily. The final directory structure is as follows:. |-- docs | |--.vuepress | | |--dist | | | |--index.html | | | |--404.html | | | |--assets | | | | |--css | | | | |--js | | | | |--img | |--README.md |-- package.json Step2. Run Dev Modenpm run docs:dev # or vuepress dev docs/ This command will start a dev environment, and you can browse the static site with localhost:8080 Step3. Build Static Sitenpm run docs:build This command will generate static files in vuepress_blog/docs/.vuepress/dist, such as: drwxr-xr-x 5 xxx staff 160B Apr 23 11:29 assets -rw-r--r-- 1 xxx staff 1.2K Apr 23 11:29 404.html -rw-r--r-- 1 xxx staff 2.1K Apr 23 11:29 index.html Step4. Config serverless.ymlcd vuepress_blog/docs/.vuepress touch serverless.yml The serverless.yml is as below:name: myblog myblog: component: &quot;@serverless/tencent-website&quot; inputs: code: src: ./dist # Upload static files index: index.html error: 404.html region: ap-hongkong bucketName: your cos bucket nameNote: The component @serverless/tencent-website is suitable for tencent cloud, you can see details in github project Step5. Serverless DeployConfig the .env file for authorization of TencentCloudcd vuepress_blog/docs/.vuepress vim .env # replace your secret_id and secret_key of tencent cloud TENCENT_APP_ID=xxx # your account id TENCENT_SECRET_ID=xxx # secret id TENCENT_SECRET_KEY=xxx # secret key TENCENT_TOKEN=xxx # access token Then execute sls to deploy static site to COS:npm run docs:deploy Meanwhile, we can integrate this command in config.js as below: Step6. Check DeploymentOpen the COS http url on browser to see the index.html. ExtensionIf you want to build a documentation system, the Vuepress will be your choice. The detail description can refer to In-Depth VuePress Tutorial: Vue-Powered Docs &amp; Blog. Add a config fileAdd a config.js in vuepress_blog/docs/.vuepress directory, for example: module.exports = { title: &apos;Welcome to TcaplusDB World&apos;, description: &apos;Just playing around&apos;, themeConfig: { nav: [ { text: &apos;Home&apos;, link: &apos;/&apos; }, { text: &apos;Blog&apos;, link: &apos;/blog/&apos; }, { text: &apos;External&apos;, link: &apos;https://cloud.tencent.com/product/tcaplusdb&apos; }, ], sidebar: [ &apos;/&apos;, &apos;/blog/&apos; ] } } Add SubdirectoryCreate blog subdirectory in vuepress_blog/docs, then touch a README.md and first-post.md in blog directory. Config Vue SupportVuepress supports flexible vue component config. Take blog for example.First, add component directory in vuepress_blog/docs/.vuepress cd vuepress_blog/docs/.vuepress mkdir component cd component touch BlogIndex.vue The BlogIndex.vue is as follows: &lt;template&gt; &lt;div&gt; &lt;div v-for=&quot;post in posts&quot;&gt; &lt;h2&gt; &lt;router-link :to=&quot;post.path&quot;&gt;{{ post.frontmatter.title }}&lt;/router-link&gt; &lt;/h2&gt; &lt;p&gt;{{ post.frontmatter.description }}&lt;/p&gt; &lt;p&gt;&lt;router-link :to=&quot;post.path&quot;&gt;Read more&lt;/router-link&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; export default { computed: { posts() { return this.$site.pages .filter(x =&gt; x.path.startsWith(&apos;/blog/&apos;) &amp;&amp; !x.frontmatter.blog_index) .sort((a, b) =&gt; new Date(b.frontmatter.date) - new Date(a.frontmatter.date)); } } } &lt;/script&gt; Second, config README.md in blog directory --- blog_index: true --- # Blog Welcome to TcaplusDB blog &lt;BlogIndex /&gt; The blog_index is used to make sure that the blog index isn’t listed in the posts. When filtering the posts in the posts computed property of the BlogIndex component. The final directory structure is as below: . ├── docs │ ├── .vuepress │ │ ├── .env │ │ ├── components │ │ │ └── BlogIndex.vue │ │ ├── config.js │ │ └── serverless.yml │ ├── README.md │ ├── blog │ │ ├── README.md │ │ └── first-post.md │ └── guide │ └── README.md ├── package-lock.json └── package.json Third, rebuild and deploy static site npm run docs:build npm run docs:deploy The demo is shown as below picture: SummaryThis post introduces how to deploy a Vuepress static site with Serverless Framework to COS of TencentCloud. It is convient for users to have their simple documentation system quickly.","link":"/2020/04/23/serverless-deploy-vuepress/"},{"title":"Multiple  Python Versions Management","text":"If you have different projects deployed in a same machine and developed with different Python versions, you may suffer this issue: how to manage a multi-version environment for Python in a same machine. This article will present you the solution to manage multiple Python versions with virtualenv. 1.Prerequisites OS version: Centos7 Python version: Python-2.7.5, Python-3.6.0 virtualenv version: 15.1.0 2. Installation2.1 Python 2.7.5 InstallationFor Centos7, the default Python version is 2.7.5, so here we don’t need to install it. 2.2 Python 3.6 Installation DownloadDownload Python3.6 from Official Website, and upload package to your target machine. Compile ./configure --prefix=/data/python3.6 make &amp;&amp; make install #load library, add python3.6 library into /etc/ld.so.conf echo &quot;/data/python3.6/lib&quot; &gt;&gt;/etc/ld.so.conf #load library again ldconfig 2.3 Virtualenv InstallationVirtualenv is a very popular component for managing multi-version environment. You can refer to detail infos from virtualenv. Installation #install pip install virtualenv Env Config #define env mkdir /root/pyenv cd /root/pyenv virtualenv --python /usr/bin/python py2env virtualenv --python /data/python3.6/bin/python3 py3env Alias ConfigIf you want to switch the virtual env quickly, the following alias config is convenient for you: #make alias in /root/.bashrc alias py3env=&apos;source /root/pyenv/py3env/bin/activate&apos; alias py2env=&apos;source /root/pyenv/py2env/bin/activate&apos; #make it work source ~/.bashrc ExperienceNow you can use py3env or py2env to activate the relevant Python version. And if you want to exit current environment, you can execute deactivate command. #switch to Python 2.7 environment [root@VM_0_14_centos ~]# py27env #exit Python 2.7 (py27env) [root@VM_0_14_centos ~]# deactivate #switch to Python 3.6 environment [root@VM_0_14_centos ~]# py36env #exit Python3.6 (py36env) [root@VM_0_14_centos ~]# deactivate [root@VM_0_14_centos ~]# If your project is based on Python 3.6, please shift the py36env environment, and install dependencies with pip. Summaryvirtualenv is a good tool to do this work, meanwhile, there is another similar tool to manage multiple versions , such as pyenv. Hope this post can help you.","link":"/2020/04/28/python-multi-version-env/"},{"title":"Export DynamoDB to S3 with AWS Data Pipeline","text":"This article introduces how to export DynamoDB to Amazon S3 with AWS Data Pipeline. 1. PrefaceThis section presents how to migrate full data from DynamoDB (AWS) to Amazon S3 with AWS Data Pipeline. 2. Resources2.1 AWS ResourcesThe below components will be used in this section. Aamazon DynamoDB AWS Data Pipeline AWS S32.2 Tencent Cloud ResourcesThe below components will be used in this section. Tcaplus Database CVM EMR COS 3. Prerequisites3.1 TencentCloud CVM Environment Apply for a TencentCloud CVM instance in related region. Check openssl-devel is exist or not, the boto3 module depends on the ssl module.yum install -y openssl-devel Check pip is exist or notyum install -y python-pip 3.2 Python InstallationThe whole execution environment will be in Python 3.6. If your machine has deployed projects with Python2, you may need to config multiple Python versions environment, you can refer to my blog Multiple Python Versions Management 3.3 Python Modules InstallLogin the CVM instance, and install Python modules with pip in Python3 environment, including awscli(&gt;=1.18.15), boto3 &gt;=(1.7.45), Faker(&gt;=0.8.16) , coscmd (latest) and cos-python-sdk-v5 (latest). pip install awscli==1.18.15 pip install boto3==1.7.45 pip install Faker==0.8.16 pip install coscmd==1.8.6.14 pip install cos-python-sdk-v5 3.4 AWS Credentials Config Configure aws cli in CVM environment, see official website In command line, create an .aws directory under /root directory and touch a credentials file, then replace aws_access_key_id and aws_secret_access_key of your aws account [default] aws_access_key_id=xxx aws_secret_access_key=xxx Set up region info in config file under .aws directory[default] region = xxx 3.5 Tencent Cloud COS Credentials ConfigConfigure cos cli in CVM environment, see COSCMD Documentation.Create a new file .cos.conf under /root directory, and add below contents in file: [common] secret_id = XXX secret_key = XXX bucket = examplebucket-1250000000 region = ap-shanghai max_thread = 5 part_size = 1 scheme = https 3.6 Data Type ComparisonThe data types between DynamoDB and TcaplusDB are different, see as below:|Amazon DynamoDB|TcaplusDB||—|—||Number| int32,uint32,int64,uint64,sint64, double,float||String|string||Boolean|bool||Binary|bytes||Sets|Array||List|Array||Map|struct| 3.7 Primary KeyIn DynamoDB, the primary key has two formats: partition key and range key partition key onlyIn TcaplusDB, the primary key includes one or more attributes(less than 4). In this section, the primary key in TcaplusDB is the same as DynamoDB, including one paritition attribute and one range attribute. Meanwhile, design an index of partition key. 3.8 Table Sample Attribute Type Note Username String primary key PointsEarned int64 ReminderDate String Subscribed Bool Zipcode int64 UserInfo String 4. DynamoDB EnvironmentThis section presents how to create DynamoDB table and make sample data.All operations are held in CVM instance. 4.1 Create DynamoDB TableIn command line, create an DynamoDB table that uses the aws dynamodb create-table command as below. aws dynamodb create-table --table-name Tcaplus_test \\ --attribute-definitions AttributeName=Username,AttributeType=S \\ --key-schema AttributeName=Username,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 4.2 Verify the table statusaws dynamodb describe-table --table-name Tcaplus_test \\ --query &apos;Table.TableStatus&apos; Check its response, ACTIVE means that table has been created successfully. 4.3 Make sample dataUse faker module in Python to make sample data for DynamoDB, the code is as below: import argparse import boto3 from faker import Faker import json import random import sys # write fake recrods to DynamoDB table to be used to demonstrate migration genFake = Faker() #dynamodb max batch size MAXBATCHSIZE = 25 client = boto3.client(&apos;dynamodb&apos;, region_name=&apos;us-east-1&apos;) def writeItems(recordCount,tableName): fullbatches = int(recordCount / MAXBATCHSIZE) partialbatch = recordCount % MAXBATCHSIZE for _ in range(fullbatches): try: response = client.batch_write_item(RequestItems={tableName: makeFakeBatch()}) unprocessed_records = len(response[&apos;UnprocessedItems&apos;].get(tableName,[])) if unprocessed_records &gt; 0: print(&apos;Unprocessed Items: %d&apos; % len(response[&apos;UnprocessedItems&apos;].get(tableName,[]))) except Exception as e: print(e) if partialbatch &gt;= 1: try: response = client.batch_write_item(RequestItems={tableName: makeFakeBatch(partialbatch)}) unprocessed_records = len(response[&apos;UnprocessedItems&apos;].get(tableName,[])) if unprocessed_records &gt; 0: print(&apos;Unprocessed Items: %d&apos; % len(response[&apos;UnprocessedItems&apos;].get(tableName,[]))) except Exception as e: print(e) def makeFakeBatch(numrecords=MAXBATCHSIZE): fake_data = [] for _ in range(numrecords): fake_record = { &quot;Username&quot;: {&quot;S&quot; : genFake.user_name()+str(random.randrange(0,10000))}, &quot;Zipcode&quot;: {&quot;N&quot; : genFake.zipcode()}, &quot;Subscribed&quot;: {&quot;BOOL&quot; : True}, &quot;ReminderDate&quot;: {&quot;S&quot; : str(genFake.past_date())}, &quot;PointsEarned&quot;: {&quot;N&quot; : str(genFake.random_int())}, &quot;UserInfo&quot;: {&quot;S&quot;: genFake.text(1024)} } fake_data.append({&quot;PutRequest&quot;:{&quot;Item&quot;: fake_record}}) return fake_data def countRecords(tblName,scan_count=0,starting_key=None): if starting_key is None: response = client.scan(TableName=tblName, Select=&apos;COUNT&apos;, ConsistentRead=True) else: response = client.scan(TableName=tblName, ConsistentRead=True, Select=&apos;COUNT&apos;, ExclusiveStartKey=starting_key) if &apos;LastEvaluatedKey&apos; in response: scan_count += response[&apos;Count&apos;] return countRecords(tblName,scan_count,response[&apos;LastEvaluatedKey&apos;]) else: scan_count += response[&apos;Count&apos;] return scan_count def main(argv): parser = argparse.ArgumentParser() parser.add_argument(&quot;--items&quot;, help=&quot;number of items to create in table&quot;, default=25000, required=False) parser.add_argument(&quot;--table&quot;, help=&quot;name of table&quot;, default=&quot;Migration&quot;, required=False) args = parser.parse_args() recordCount = int(args.items) tableName = args.table writeItems(recordCount,tableName) print(&apos;Items in Table: %d&apos;% countRecords(tableName)) if __name__ == &quot;__main__&quot;: main(sys.argv[1:]) Execute the Python script in CVM as below. python make_sample_data.py --table Tcaplus_test --items 10000 5. TcaplusDB Environment5.1 Create TcaplusDB Table In TencentCloud TcaplusDB Console, create an TcaplusDB table, see official websiteTcaplusDB table protobuf (v3) file as below: syntax = &quot;proto3&quot;; import &quot;tcaplusservice.optionv1.proto&quot;; message Tcaplus_test { option(tcaplusservice.tcaplus_primary_key) = &quot;Username&quot;; string Username = 1; int64 PointsEarned = 2; string ReminderDate = 3; bool Subscribed = 4; int64 Zipcode = 5; string UserInfo = 6; } 6. Migrate Data to S36.1. Create S3 BucketCreate an Amazon S3 bucket to receive the DynamoDB export. aws s3 mb s3://2-dynamodb-tcaplus-export 6.2. Create AWS Data PipelineBefore you start, refer to Getting Started with AWS Data Pipeline Source Select Build using a template Choose Export DynamoDB table to S3 Paramters Source DynamoDB table name: Tcaplus_test Output S3 folder: s3://2-dynamodb-tcaplus-export/Tcaplus_test/ DynamoDB read throughput ratio: 0.25 Region of the DynamoDB table: us-east-1 Schedule Select on pipeline activation Pipeline Configuration Disable Logging , Enable it if necessary Security/Access IAM roles select Default 6.3. Migrate S3 file to TencentCloud COSThis section introduces two approaches to migrate S3 file to COS Command Line Mode Programming Mode6.3.1 Command Line Mode Step1, login the CVM instance with ssh Step2, create a directory (migration_tool) under /root directory Step3, go into migration_tool directory and create a sub directory tcaplus_test Step4, download table file from s3 to local tcaplus_test directory cd /root/migration_tool aws s3 cp s3://2-dynamodb-tcaplus-export/Tcaplus_test/2020-03-11-10-29-13 tcaplus_test/ --recursive Step5, upload file from local directory to COS directory cd /root/migration_tool coscmd -b tcaplus-sh-cos-1258272208 upload tcaplus_test Tcaplus_test/2020-03-11-10-29-13 -r 6.3.2 Programming ModeBoth S3 and COS provide Python SDK to operate objects, it’s easy to write Python script. Step1, login the CVM instance with ssh Step2, install tencentcloud cos package with pip install -U cos-python-sdk-v5 under Python3 Environment Step2, create a directory (migration_tool) under /root directory Step3, go into migration_tool directory , create a Python Script named s3_to_cos.py as below:```#!/usr/bin/env python-*- coding=utf-8from qcloud_cos import CosConfigfrom qcloud_cos import CosS3Clientimport boto3#table name of migrationdynamodb_table=’Tcaplus_test’tcaplusdb_table=’Tcaplus_test’ s3_bucket_name=’2-dynamodb-tcaplus-export’cos_bucket_name = ‘tcaplus-sh-cos-1258272208’ s3_prefix=”Tcaplus_test/2020-03-11-10-29-13”cos_prefix=”Tcaplus_test/2020-03-11-10-29-13” #init S3 clients3RegionName = “us-east-1”s3Client = boto3.client(“s3”,region_name=s3RegionName) scheme=”https”cosRegionName=”ap-shanghai” #replace your api credentials of your accountSECRET_ID=’xxx’SECRET_KEY = ‘xxx’ #init cos clientcos_config = CosConfig(Region=cosRegionName, SecretId=SECRET_ID, SecretKey=SECRET_KEY,Scheme=scheme)cosClient = CosS3Client(cos_config) def put_s3_to_cos(): “”” get s3 objects and put objects to cos :returns response “”” try: #list S3 objects of S3 bucket with prefix filter s3_resp = s3Client.list_objects_v2( Bucket=s3_bucket_name, Prefix=s3_prefix, MaxKeys=1024 ) print(s3_resp) s3_objects = s3_resp[&apos;Contents&apos;] #iterate the object list , migrate one object each time #Note: if object size hits limit up (aws:5TB, tencent:5GB), you should use another interfaces ,see official website for s3_object in s3_objects: obj_content = s3Client.get_object(Bucket=s3_bucket_name,Key=s3_object[&apos;Key&apos;]) obj_stream = obj_content[&apos;Body&apos;] cos_key = s3_object[&apos;Key&apos;] cos_resp = cosClient.put_object(Bucket=cos_bucket_name,Body=obj_stream,Key=cos_key) print(cos_resp) print(&quot;s3 migrates to cos finish&quot;) except Exception as e: print(&quot;s3 migrates to cos fail &quot;,e) if name == ‘main‘: put_s3_to_cos() ## 6.4 Migrate COS Data to TcaplusDB ### 6.4.1 Prerequisites * Prepare for TcaplusDB Python Restful SDK Please refer to `TcaplusDB Essential` document. The SDK will be provided as attachment, and add this SDK to `/root/migrate_tool` directory. * Download COS files from COS bucket to local directory of CVM cd /root/migration_toolcoscmd -b tcaplus-sh-cos-1258272208 download Tcaplus_test/2020-03-11-10-29-13/ sample_data/ -r ### 6.4.2 Data Type The row format of COS file is low-level data type of DynamoDB. First, it must be converted to high level data. * Low-level data {“a”:{“S”:”test”}, “b”:{“N”:”12”}, “c”:{“bOOL”:true}} * High-level data { “a” : “test” , “b” : 12, “c” : True} ### 6.4.3 Convert and Save * Sample Data {“Username”:{“s”:”kevinortega9345”},”Subscribed”:{“bOOL”:true},”Zipcode”:{“n”:”86120”},”ReminderDate”:{“s”:”2020-02-07”},”PointsEarned”:{“n”:”4351”} }{“Username”:{“s”:”ashleywilson3807”},”Subscribed”:{“bOOL”:true},”Zipcode”:{“n”:”20612”},”ReminderDate”:{“s”:”2020-03-02”},”PointsEarned”:{“n”:”4121”}}{“Username”:{“s”:”frazierfrank8872”},”Subscribed”:{“bOOL”:true},”Zipcode”:{“n”:”9456”},”ReminderDate”:{“s”:”2020-02-17”},”PointsEarned”:{“n”:”7896”}}{“Username”:{“s”:”eric383371”},”Subscribed”:{“bOOL”:true},”Zipcode”:{“n”:”7748”},”ReminderDate”:{“s”:”2020-02-25”},”PointsEarned”:{“n”:”6303”}} * Convert and Save Script Create a Python Script in `/root/migration_tool` directory, the code is as below: import jsonimport boto3from tcaplus.tcaplusdb_rest_client import TcaplusRestClient class CustomEncoder(json.JSONEncoder): def default(self, obj): try: if isinstance(obj, bytes): return str(obj, encoding=’utf-8’) elif isinstance(obj, decimal.Decimal): if obj == int(obj): return int(obj) else: return float(obj) elif isinstance(obj, set): return list(obj) # Let the base class default method raise the TypeError return json.JSONEncoder.default(self, obj) except Exception as e: return obj.__dict__[&apos;value&apos;] #dynamodb deserializerboto3.resource(“dynamodb”)deserializer = boto3.dynamodb.types.TypeDeserializer() #TcaplusDB client #TcaplusDB endpoint, internal ip addressendpoint=”http://x.x.x.x“ #TcaplusDB access id of table clusteraccessId = xxx #TcaplusDB access password of table clusteraccessPasswd = “xxxx” #TcaplusDB table groupo idtableGroupId = xxx #TcaplusDB table nametableName = “Tcaplus_test” #init TcaplusDB clientclient = TcaplusRestClient(endpoint, accessId, accessPasswd)client.SetTargetTable(tableGroupId, tableName) def convert(filename): with open(filename) as f: lines = f.readlines() for line in lines: #replace all irregular values nline = line.replace(&apos;:false&apos;,&apos;:&quot;False&quot;&apos;).replace(&apos;:true&apos;,&apos;:&quot;True&quot;&apos;).replace(&apos;bOOL&apos;,&apos;BOOL&apos;).replace(&apos;nULLValue&apos;,&apos;NULL&apos;) lowLevelLine = json.loads(nline) highLevelLine = {k: deserializer.deserialize(v) for k,v in lowLevelLine.items()} jsonStr = json.dumps(highLevelLine,cls=CustomEncoder) print(jsonStr) # add record to TcaplusDB record = json.loads(jsonStr) status, resp = client.AddRecord(record) print(resp) if name == ‘main‘: #replace your file name of sample data here filename=os.getcwd()+&apos;/sample_data/c54a3412-44ba-4e69-b854-58050e133337&apos; convert(filename) ## 6.5. Summary This section presents a simple way to migrate data from COS to TcaplusDB. However, if the data size is over 1GB or 10+GB, the performance will be unacceptable. For big data scenarios, parallelism is important. There are lots of big data suites such as Spark, Map/Reduce for this scenario. Here, the EMR product of TencentCloud is very suitable for processing data in parallel. Meanwhile, the EMR has gotten through COS, and user can use the EMR to process COS data directly. The whole steps will be introduced in next paper. # 7. Attachment * tcaplusdb-restapi-python-sdk-en https://git.code.oa.com/tcaplus/tcaplusdb-restapi-python-sdk-en```Include 8 basic API interfaces to operate TcaplusDB data","link":"/2020/04/28/dynamodb-export-with-pipeline/"}],"tags":[{"name":"Airflow","slug":"Airflow","link":"/tags/Airflow/"},{"name":"DynamoDB,COS","slug":"DynamoDB-COS","link":"/tags/DynamoDB-COS/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"vuepress,serverless","slug":"vuepress-serverless","link":"/tags/vuepress-serverless/"},{"name":"virtualenv, python","slug":"virtualenv-python","link":"/tags/virtualenv-python/"},{"name":"DynamoDB, S3","slug":"DynamoDB-S3","link":"/tags/DynamoDB-S3/"}],"categories":[{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"TencentCloud","slug":"TencentCloud","link":"/categories/TencentCloud/"},{"name":"HBase","slug":"HBase","link":"/categories/HBase/"},{"name":"Serverless","slug":"Serverless","link":"/categories/Serverless/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"AWS","slug":"AWS","link":"/categories/AWS/"}]}