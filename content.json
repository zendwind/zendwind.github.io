{"pages":[{"title":"About Me","text":"Welcome to my blog.It’s ballen, a cloud compute solution engineer in Tencent. I am interested in various database products, such as MySQL, HBase, Redis, MongoDB, DynamoDB, CosmosDB, Tencent TcaplusDB, etc. I like recording whatever I see and think. Four years ago, I started engaging in DBA, and aimed at solving various problems of HBase/Hadoop. Meanwhile, learned how to be a competent DevOps engineer. Two years ago, I focused on developing fincinal backend system, and learned how to desgin a data-intensive application with micro service framework. It’s a great opportunity for me to connect with the wonderful Tech World. Hope your guys can enjoy it and feel free to contact me if you have any questions. Nickname: ballenEmail: zendwind@gmail.comGithub: @zendwind","link":"/about/index.html"}],"posts":[{"title":"How to Deploy Airflow with Docker","text":"This post introduces how to deploy Airflow with Docker. Prerequisites CentOS7 (64-bit) TencentCloud CVM (4C,8G) Python3.6.0Docker InstallDocker Installyum install -y docker #in centos8, yum install -y docker-ce systemctl start docker Docker Compose Install # download from github with curl curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose #install python module pip install docker-compose #check docker-compose version docker-compose version Airflow Docker Install Pull from Repositorydocker pull puckel/docker-airflow Download Git Packagegit clone https://github.com/puckel/docker-airflow.git Airflow LocalExecutor DeployIntroductionThe Airflow supports various deployment mode, such as SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor. Airflow Local Executor ModeRunning Airflow on a LocalExecutor exemplifies single-node architecture. In dev or test environment, this mode is simple and easy for users to use Airflow. Airflow CeleryExecutor ModeRunning Airflow on a CeleryExecutor exemplifies distributed architecture, and communicates with Celery. For production environment, this mode is popular for users to deploy. In this solution, only introduce how to deploy a LocalExecutor with docker. LocalExecutor Deploy Step 1. Modify airflow.cfgGo into docker-airflow/config directory, open airflow.cfg and modify two items as below:```#specify executor typeexecutor = LocalExecutor #specify postgresql data sourcesql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow * Step2. Modify Dockerfile The dockerfile in puckel image is based on Debian and includes all of steps how to build the airflow docker image. So it&apos;s unnecessary for you to rebuild the whole image, the simplified dockerfile is as below, which only contains basic steps. FROM puckel/docker-airflow:latest #RUN pip install –user psycopg2-binaryARG AIRFLOW_USER_HOME=/usr/local/airflowUSER root #RUN pip install xxxENV AIRFLOW_HOME=/usr/local/airflowCOPY config/airflow.cfg /usr/local/airflow/airflow.cfg RUN chown -R airflow: ${AIRFLOW_USER_HOME} EXPOSE 8080 5555 8793 USER airflowWORKDIR ${AIRFLOW_USER_HOME}ENTRYPOINT [“/entrypoint.sh”]CMD [“webserver”] * Step3. Modify docker-compose-LocalExecutor.yml This file includes the containers we need to deploy, such as postgres and webserver. Meanwhile, specify the `volumes` for container. version: ‘3.7’services: postgres: image: postgres:9.6 environment: - POSTGRES_USER=airflow - POSTGRES_PASSWORD=airflow - POSTGRES_DB=airflow ports: - &quot;5432:5432&quot; volumes: - ./data/postgres:/var/lib/postgresql/data logging: options: max-size: 10m max-file: &quot;3&quot; webserver: image: puckel/docker-airflow:latest restart: always depends_on: - postgres environment: - LOAD_EX=n - EXECUTOR=Local logging: options: max-size: 10m max-file: &quot;3&quot; volumes: - ./dags:/usr/local/airflow/dags # - ./plugins:/usr/local/airflow/plugins ports: - &quot;8080:8080&quot; command: webserver healthcheck: test: [&quot;CMD-SHELL&quot;, &quot;[ -f /usr/local/airflow/airflow-webserver.pid ]&quot;] interval: 30s timeout: 30s retries: 3 __Note__: the image version of webserver need change to `latest` version * Step4. Rebuild image cd puckel-docker-airflowdocker build –rm -t puckel/docker-airflow . * Step5. Deploy container cd puckel-docker-airflowdocker-compose -f docker-compose-LocalExecutor.yml up -d Then you can execute `docker ps` to get the container list. If you want to restart container, such as webserver, the command is as below: docker-compose -f ./docker-compose-LocalExecutor.yml stop webserverdocker-compose -f ./docker-compose-LocalExecutor.yml start webserver * Step6. Deploy Dag Add a dag file (Python) in `dags` directory which is defined as volumes of `webserver` container in docker-compose-LocalExecutor.yml. Then, you can find it on the Airflow website. http://localhost:8080 # FAQ 1. `airflow initdb` error: airflow.exceptions.AirflowException: Could not create Fernet object: Incorrect padding python -c “from cryptography.fernet import Fernet;print(Fernet.generate_key().decode())”3vWG_GoK_2MtU1fqRwCKrfNVsKW-ZcpdP8Ltz51xm64= export AIRFLOWCOREFERNET_KEY=3vWG_GoK_2MtU1fqRwCKrfNVsKW-ZcpdP8Ltz51xm64=```","link":"/2020/04/04/airflow-docker-install/"},{"title":"How to Migrate DynamoDB to Another Platforms","text":"This post aims to describe how to migrate DyanmoDB data to other Platform. The whole post will include four subtopics as below: Part One: Export DynamoDB Data to S3 with AWS Data Pipeline Part Two: Import S3 Data to Tencent TcaplusDB Part Three: Principles of Design A Migration System Part Four: DynamoDB Real-time Migration Solution Why Migrate?Different customers have different cases. As for DynamoDB, it is a popular NoSQL product around the world, and holds most of the market share. So why need to migrate data from DynamoDB to other Platforms. I think the leading causes are as below: PriceAs we known, DynamoDB has two pricing mode: on-demand and provisioned. Both of them are cost-expensive, if your business has large traffic in read/write, the DynamoDB will occupy a large proportion. IntegrationThe DynamoDB is strongly bound with AWS, which means you must use AWS other products to work with DynamoDB. I think it is not a good news for most of customers who are unwilling to be tied on one boat. Export ModeThere are three approaches for exporting DynamoDB data: Export to .csv from DynamoDB PortalThis mode is the most easiest way to export data, however it is just available for those small tables. If table has massive data, this way will be heavy workload, because only 100 items can be exported each time. Export with ScanThis mode is the most popular way to export all of DynamoDB data, however, it is cost-expensive and will exhaust a large number of RCU. Meanwhile, if you use Scan to export data , you must consider how to avoid impacting online business, such as traffic control, bandwidth control, maybe the third-party tool need to be introduced to limit the migrating rate (Google Guava RateLimit). Export with Data PipelineThis mode is the most cost-effective way to export all of DynamoDB data. It uses AWS EMR to process off-line data parallelly, and its core concept is Map/Reduce. With this offline mode, the scope of business impact is small. In next post, we will use this approach to finish the migrating work. Get Started with Migration Part One: Export DynamoDB Data to S3 with AWS Data Pipeline Part Two: Import S3 Data to Tencent TcaplusDB Part Three: Principles of Design A Migration System Part Four: DynamoDB Real-time Migration Solution","link":"/2020/03/28/dynamodb-migrate-s3/"},{"title":"An Introduction to HBase Native API","text":"HBase is a distributed database, this article will introduce HBase Native API to help everyone have a preliminary understanding of HBase API. HBase intruductionHBase is a distributed database , which can support random/sequential write/read of batch data.HBase architecture is shown in the following figure. HBase Master is the key component, in charge of the whole cluster management, such as node heartbeat, meta data, load balance, fault tolerance, and region management. Client is in charge of the communication between application and hbase. Zookeeper is a distributed coordination component, in charge of master selection and cluster status management. Regionserver is the node of cluster, in charge of data storage, region management, write/read request response, .e.g. HBase data model is shown in the following table. As we can know from above figure, the model includes four pieces, rowkey, timestamp, column family and qualifer. Rowkey is the unique key of hbase, each record in hbase has its own timestamp to identify the time of insert or update; CF(Column Family) consists of serveral similar columns(qualifiers), different attributes of column distribute different cf in order to manage more conveniently. HBase Shell APIHow to get row data from hbase? HBase provides two kinds of api to complete it, scan and get. The GET method aims to get one row by rowkey each time, nevertheless the SCAN method aims to get multi rows by rowkey-prefix each time. Next, we will introduce GET and SCAN method. First, Get is the standard api of hbase. If we want to get r1 rowkey data by rowkey, the command in hbase shell will like this: get &apos;table&apos;,&apos;r1&apos; get &apos;table&apos;, &apos;r1&apos;, &apos;cf:a&apos; The first command will return all columns data (cf:a, cf:b), and the second command will only return &apos;cf:a&apos; data. Second, Scan is also the standard api of hbase. If we want to get both &apos;r1&apos; and &apos;r2&apos; data, and hypothesis both of them have common rowkey-prefix,then we can use scan like this: scan &apos;table&apos;,{STARTROW=&gt;&apos;row-start-prefix&apos;,ENDROW=&gt;&apos;row-end-prefix&apos;} As we can see this command, we must specify two parameters STARTROW and ENDROW, which symbolize the rowkey data range of scan in hbase, if we only need specified number of lines, the other parameter can help us to do this, like this: scan &apos;table&apos;,{STARTROW=&gt;&apos;row-start-prefix&apos;, ENDROW=&gt;&apos;row-end-prefix&apos;,LIMIT=&gt;2} The LIMIT instruction tells hbase to scan the limit rows and then return result. If we need return specified value of row, we can use filter to help us, like this: scan &apos;table&apos;,{STARTROW=&gt;&apos;row-start-prefix&apos;,ENDROW=&gt;&apos;row-end-prefix&apos;,FILTER=&gt;&quot;SingleColumnValueFilter(&apos;cf&apos;,&apos;a&apos;,=,&apos;binary:1&apos;)&quot;} This command will return all rows of &apos;cf:a&apos; equals 1 . There are also many other filter method in hbase, such PrefixFilter, CompareFilter and so on. In future articles , I will introduce more about filters knowledge. ConclusionIn this paper, I only introduce the simple hbase shell api GET and SCAN. In next paper, I will start to introduce more about java api to get hbase data.","link":"/2018/10/22/hbase-api/"},{"title":"How to Deploy Vuepress with Serverless Framework","text":"Serverless Framework is very popular all over the world, and has 30,000+ stars in Github. It is friendly for many cloud platforms, such as AWS, GCP, Azure, TencentCloud, AliCloud,etc.This post will present how to deploy vuepress (a simple static site generator) with Serverless Framework on TencentCloud. PrerequisitesNodeJS and NPM InstallThe installation can refer to How to Install Node.js and NPM on a Mac. Serverless Installnpm install -g serverless Vuepress Installnpm install -g vuepress COS Bucket Create Apply for TencentCloud account Check out the COS documentation page for an example of creating a COS bucket in relevant region. IntroductionServerless FrameworkThe introduction about serverless can refer to official website. VuepressVuepress includes two parts: one is minimalistic static site generator, the other is theme. The detail informations can refer to official website. Best PracticeOperation StepsStep1.Project Preparationmkdir -p vuepress_blog/docs cd vuepress_blog touch package.json touch docs/README.md echo &quot;Hello, My first Vuepress Blog&quot; &gt;&gt;README.md The package.json is as below:{ &quot;scripts&quot;: { &quot;docs:dev&quot;: &quot;vuepress dev docs&quot;, &quot;docs:build&quot;: &quot;vuepress build docs&quot;, &quot;docs:deploy&quot;: &quot;cd docs/.vuepress &amp;&amp; sls&quot; } } docs:dev : start a dev environment docs:build : build a static site docs:deploy : deploy the static site to cos with Serverless Framework Note: With package.json, you can integrate your project with CI service easily. The final directory structure is as follows:. |-- docs | |--.vuepress | | |--dist | | | |--index.html | | | |--404.html | | | |--assets | | | | |--css | | | | |--js | | | | |--img | |--README.md |-- package.json Step2. Run Dev Modenpm run docs:dev # or vuepress dev docs/ This command will start a dev environment, and you can browse the static site with localhost:8080 Step3. Build Static Sitenpm run docs:build This command will generate static files in vuepress_blog/docs/.vuepress/dist, such as: drwxr-xr-x 5 xxx staff 160B Apr 23 11:29 assets -rw-r--r-- 1 xxx staff 1.2K Apr 23 11:29 404.html -rw-r--r-- 1 xxx staff 2.1K Apr 23 11:29 index.html Step4. Config serverless.ymlcd vuepress_blog/docs/.vuepress touch serverless.yml The serverless.yml is as below:name: myblog myblog: component: &quot;@serverless/tencent-website&quot; inputs: code: src: ./dist # Upload static files index: index.html error: 404.html region: ap-hongkong bucketName: your cos bucket nameNote: The component @serverless/tencent-website is suitable for tencent cloud, you can see details in github project Step5. Serverless DeployConfig the .env file for authorization of TencentCloudcd vuepress_blog/docs/.vuepress vim .env # replace your secret_id and secret_key of tencent cloud TENCENT_APP_ID=xxx # your account id TENCENT_SECRET_ID=xxx # secret id TENCENT_SECRET_KEY=xxx # secret key TENCENT_TOKEN=xxx # access token Then execute sls to deploy static site to COS:npm run docs:deploy Meanwhile, we can integrate this command in config.js as below: Step6. Check DeploymentOpen the COS http url on browser to see the index.html. ExtensionIf you want to build a documentation system, the Vuepress will be your choice. The detail description can refer to In-Depth VuePress Tutorial: Vue-Powered Docs &amp; Blog. Add a config fileAdd a config.js in vuepress_blog/docs/.vuepress directory, for example: module.exports = { title: &apos;Welcome to TcaplusDB World&apos;, description: &apos;Just playing around&apos;, themeConfig: { nav: [ { text: &apos;Home&apos;, link: &apos;/&apos; }, { text: &apos;Blog&apos;, link: &apos;/blog/&apos; }, { text: &apos;External&apos;, link: &apos;https://cloud.tencent.com/product/tcaplusdb&apos; }, ], sidebar: [ &apos;/&apos;, &apos;/blog/&apos; ] } } Add SubdirectoryCreate blog subdirectory in vuepress_blog/docs, then touch a README.md and first-post.md in blog directory. Config Vue SupportVuepress supports flexible vue component config. Take blog for example.First, add component directory in vuepress_blog/docs/.vuepress cd vuepress_blog/docs/.vuepress mkdir component cd component touch BlogIndex.vue The BlogIndex.vue is as follows: &lt;template&gt; &lt;div&gt; &lt;div v-for=&quot;post in posts&quot;&gt; &lt;h2&gt; &lt;router-link :to=&quot;post.path&quot;&gt;{{ post.frontmatter.title }}&lt;/router-link&gt; &lt;/h2&gt; &lt;p&gt;{{ post.frontmatter.description }}&lt;/p&gt; &lt;p&gt;&lt;router-link :to=&quot;post.path&quot;&gt;Read more&lt;/router-link&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; export default { computed: { posts() { return this.$site.pages .filter(x =&gt; x.path.startsWith(&apos;/blog/&apos;) &amp;&amp; !x.frontmatter.blog_index) .sort((a, b) =&gt; new Date(b.frontmatter.date) - new Date(a.frontmatter.date)); } } } &lt;/script&gt; Second, config README.md in blog directory --- blog_index: true --- # Blog Welcome to TcaplusDB blog &lt;BlogIndex /&gt; The blog_index is used to make sure that the blog index isn’t listed in the posts. When filtering the posts in the posts computed property of the BlogIndex component. The final directory structure is as below: . ├── docs │ ├── .vuepress │ │ ├── .env │ │ ├── components │ │ │ └── BlogIndex.vue │ │ ├── config.js │ │ └── serverless.yml │ ├── README.md │ ├── blog │ │ ├── README.md │ │ └── first-post.md │ └── guide │ └── README.md ├── package-lock.json └── package.json Third, rebuild and deploy static site npm run docs:build npm run docs:deploy The demo is shown as below picture: SummaryThis post introduces how to deploy a Vuepress static site with Serverless Framework to COS of TencentCloud. It is convient for users to have their simple documentation system quickly.","link":"/2020/04/23/serverless-deploy-vuepress/"},{"title":"Multiple  Python Versions Management","text":"If you have different projects deployed in a same machine and developed with different Python versions, you may suffer this issue: how to manage a multi-version environment for Python in a same machine. This article will present you the solution to manage multiple Python versions with virtualenv. 1.Prerequisites OS version: Centos7 Python version: Python-2.7.5, Python-3.6.0 virtualenv version: 15.1.0 2. Installation2.1 Python 2.7.5 InstallationFor Centos7, the default Python version is 2.7.5, so here we don’t need to install it. 2.2 Python 3.6 Installation DownloadDownload Python3.6 from Official Website, and upload package to your target machine. Compile ./configure --prefix=/data/python3.6 make &amp;&amp; make install #load library, add python3.6 library into /etc/ld.so.conf echo &quot;/data/python3.6/lib&quot; &gt;&gt;/etc/ld.so.conf #load library again ldconfig 2.3 Virtualenv InstallationVirtualenv is a very popular component for managing multi-version environment. You can refer to detail infos from virtualenv. Installation #install pip install virtualenv Env Config #define env mkdir /root/pyenv cd /root/pyenv virtualenv --python /usr/bin/python py2env virtualenv --python /data/python3.6/bin/python3 py3env Alias ConfigIf you want to switch the virtual env quickly, the following alias config is convenient for you: #make alias in /root/.bashrc alias py3env=&apos;source /root/pyenv/py3env/bin/activate&apos; alias py2env=&apos;source /root/pyenv/py2env/bin/activate&apos; #make it work source ~/.bashrc ExperienceNow you can use py3env or py2env to activate the relevant Python version. And if you want to exit current environment, you can execute deactivate command. #switch to Python 2.7 environment [root@VM_0_14_centos ~]# py27env #exit Python 2.7 (py27env) [root@VM_0_14_centos ~]# deactivate #switch to Python 3.6 environment [root@VM_0_14_centos ~]# py36env #exit Python3.6 (py36env) [root@VM_0_14_centos ~]# deactivate [root@VM_0_14_centos ~]# If your project is based on Python 3.6, please shift the py36env environment, and install dependencies with pip. Summaryvirtualenv is a good tool to do this work, meanwhile, there is another similar tool to manage multiple versions , such as pyenv. Hope this post can help you.","link":"/2020/04/28/python-multi-version-env/"},{"title":"Export DynamoDB to S3 with AWS Data Pipeline","text":"This article presents how to migrate full data from AWS DynamoDB to Amazon S3 with AWS Data Pipeline. 1. PrefaceDynamoDB migration is a hard work. This post will introduce the migrating procedures, including table creation, data falsification, data exportation, and so on. 2. Prerequisites2.1 AWS ResourcesThe below components will be used in this section. Aamazon DynamoDB AWS Data Pipeline AWS S3 2.2 AWS CredentialsConfigure AWS credentials in your host, refer to official website. Create a .aws directory in your home directory, such as /root/.aws, touch a credentials file in .aws direcotry , and replace aws_access_key_id and aws_secret_access_key of your aws account. [default] aws_access_key_id=xxx aws_secret_access_key=xxx Set up region info in config file under .aws directory [default] region = xxx 2.3 DependenciesThis article depends on some AWS tools and other tools, such as awscli, boto3, and Faker. The Python version is 2.7.10: CentOS:pip install awscli pip install boto3 pip install Faker MacOS:pip install awscli --user pip install boto3 --user pip install Faker --user Note: The boto3 module depends on ssl support, so if your Python environment is no ssl module, you should install it first and then re-install the Python. Type import ssl in Python command line to verify whether it is ok or not. 2.4 DynamoDB Table Definition Attribute Type Note Username String primary key PointsEarned int64 ReminderDate String Subscribed Bool Zipcode int64 UserInfo String 3. DynamoDB Environment3.1 Create DynamoDB TableIn command line, create an DynamoDB table that uses the aws dynamodb create-table command as below. aws dynamodb create-table --table-name Dynamodb_test \\ --attribute-definitions AttributeName=Username,AttributeType=S \\ --key-schema AttributeName=Username,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 3.2 Verify Table Statusaws dynamodb describe-table --table-name Dynamodb_test \\ --query &apos;Table.TableStatus&apos; The ACTIVE response means that table has been created successfully. 3.3 Fake Sample DataUse faker module in Python to make sample data for DynamoDB, the code is as below: import argparse import boto3 from faker import Faker import json import random import sys # write fake recrods to DynamoDB table to be used to demonstrate migration genFake = Faker() #dynamodb max batch size MAXBATCHSIZE = 25 client = boto3.client(&apos;dynamodb&apos;, region_name=&apos;us-east-1&apos;) def writeItems(recordCount,tableName): fullbatches = int(recordCount / MAXBATCHSIZE) partialbatch = recordCount % MAXBATCHSIZE for _ in range(fullbatches): try: response = client.batch_write_item(RequestItems={tableName: makeFakeBatch()}) unprocessed_records = len(response[&apos;UnprocessedItems&apos;].get(tableName,[])) if unprocessed_records &gt; 0: print(&apos;Unprocessed Items: %d&apos; % len(response[&apos;UnprocessedItems&apos;].get(tableName,[]))) except Exception as e: print(e) if partialbatch &gt;= 1: try: response = client.batch_write_item(RequestItems={tableName: makeFakeBatch(partialbatch)}) unprocessed_records = len(response[&apos;UnprocessedItems&apos;].get(tableName,[])) if unprocessed_records &gt; 0: print(&apos;Unprocessed Items: %d&apos; % len(response[&apos;UnprocessedItems&apos;].get(tableName,[]))) except Exception as e: print(e) def makeFakeBatch(numrecords=MAXBATCHSIZE): fake_data = [] for _ in range(numrecords): fake_record = { &quot;Username&quot;: {&quot;S&quot; : genFake.user_name()+str(random.randrange(0,10000))}, &quot;Zipcode&quot;: {&quot;N&quot; : genFake.zipcode()}, &quot;Subscribed&quot;: {&quot;BOOL&quot; : True}, &quot;ReminderDate&quot;: {&quot;S&quot; : str(genFake.past_date())}, &quot;PointsEarned&quot;: {&quot;N&quot; : str(genFake.random_int())}, &quot;UserInfo&quot;: {&quot;S&quot;: genFake.text(1024)} } fake_data.append({&quot;PutRequest&quot;:{&quot;Item&quot;: fake_record}}) return fake_data def countRecords(tblName,scan_count=0,starting_key=None): if starting_key is None: response = client.scan(TableName=tblName, Select=&apos;COUNT&apos;, ConsistentRead=True) else: response = client.scan(TableName=tblName, ConsistentRead=True, Select=&apos;COUNT&apos;, ExclusiveStartKey=starting_key) if &apos;LastEvaluatedKey&apos; in response: scan_count += response[&apos;Count&apos;] return countRecords(tblName,scan_count,response[&apos;LastEvaluatedKey&apos;]) else: scan_count += response[&apos;Count&apos;] return scan_count def main(argv): parser = argparse.ArgumentParser() parser.add_argument(&quot;--items&quot;, help=&quot;number of items to create in table&quot;, default=25000, required=False) parser.add_argument(&quot;--table&quot;, help=&quot;name of table&quot;, default=&quot;Migration&quot;, required=False) args = parser.parse_args() recordCount = int(args.items) tableName = args.table writeItems(recordCount,tableName) print(&apos;Items in Table: %d&apos;% countRecords(tableName)) if __name__ == &quot;__main__&quot;: main(sys.argv[1:]) Execute the Python script in command line as below. python make_sample_data.py --table Dynamodb_test --items 10000 4. Migrate Data to S34.1. Create S3 BucketCreate an Amazon S3 bucket to receive the DynamoDB export. aws s3 mb s3://2-dynamodb-tcaplus-export 4.2. Create AWS Data PipelineBefore you start, refer to Getting Started with AWS Data Pipeline. The sample configuration of data pipeline is as below: Source Select Build using a template Choose Export DynamoDB table to S3 Paramters Source DynamoDB table name: Dynamodb_test Output S3 folder: s3://2-dynamodb-tcaplus-export/Dynamodb_test/ DynamoDB read throughput ratio: 0.25 Region of the DynamoDB table: us-east-1 Schedule Select on pipeline activation Pipeline Configuration Disable Logging , Enable it if necessary Security/Access IAM roles select Default The above configuration is shown in following figure. 4.3 Activate Data PipelineAfter configuring the pipeline, activate the pipeline and you will see below information: 4.4 Check StatusIf the Data pipeline has been scheduled successfully, it will execute the exportation step by step. When the workflow has shown the HEALTHY status, it means the exporting work is done, otherwise the Health Status will be ERROR. 4.5 Check DataThe Data Pipeline uses map/reduce technology to transform data parallelly. And the result file of exportation in S3 bucket is like below. {&quot;Username&quot;:{&quot;s&quot;:&quot;kevinortega9345&quot;},&quot;Subscribed&quot;:{&quot;bOOL&quot;:true},&quot;Zipcode&quot;:{&quot;n&quot;:&quot;86120&quot;},&quot;ReminderDate&quot;:{&quot;s&quot;:&quot;2020-02-07&quot;},&quot;PointsEarned&quot;:{&quot;n&quot;:&quot;4351&quot;} } {&quot;Username&quot;:{&quot;s&quot;:&quot;ashleywilson3807&quot;},&quot;Subscribed&quot;:{&quot;bOOL&quot;:true},&quot;Zipcode&quot;:{&quot;n&quot;:&quot;20612&quot;},&quot;ReminderDate&quot;:{&quot;s&quot;:&quot;2020-03-02&quot;},&quot;PointsEarned&quot;:{&quot;n&quot;:&quot;4121&quot;}} {&quot;Username&quot;:{&quot;s&quot;:&quot;frazierfrank8872&quot;},&quot;Subscribed&quot;:{&quot;bOOL&quot;:true},&quot;Zipcode&quot;:{&quot;n&quot;:&quot;9456&quot;},&quot;ReminderDate&quot;:{&quot;s&quot;:&quot;2020-02-17&quot;},&quot;PointsEarned&quot;:{&quot;n&quot;:&quot;7896&quot;}} {&quot;Username&quot;:{&quot;s&quot;:&quot;eric383371&quot;},&quot;Subscribed&quot;:{&quot;bOOL&quot;:true},&quot;Zipcode&quot;:{&quot;n&quot;:&quot;7748&quot;},&quot;ReminderDate&quot;:{&quot;s&quot;:&quot;2020-02-25&quot;},&quot;PointsEarned&quot;:{&quot;n&quot;:&quot;6303&quot;}} 5. SummaryThis section presents a simple way to migrate data from DynamoDB to S3. However, if the data size is over 1GB or 10+GB, the performance will be worse, the crucial bottleneck is the transformation speed of DynamoDB data with Data Pipeline, you need consider the optimization measures for accelerating the transformation speed, such as using better performance core instance type and increasing the number of core instance count.","link":"/2020/04/28/dynamodb-export-with-pipeline/"}],"tags":[{"name":"Airflow","slug":"Airflow","link":"/tags/Airflow/"},{"name":"DynamoDB,COS","slug":"DynamoDB-COS","link":"/tags/DynamoDB-COS/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"vuepress,serverless","slug":"vuepress-serverless","link":"/tags/vuepress-serverless/"},{"name":"virtualenv, python","slug":"virtualenv-python","link":"/tags/virtualenv-python/"},{"name":"DynamoDB, S3, Faker","slug":"DynamoDB-S3-Faker","link":"/tags/DynamoDB-S3-Faker/"}],"categories":[{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"TencentCloud","slug":"TencentCloud","link":"/categories/TencentCloud/"},{"name":"HBase","slug":"HBase","link":"/categories/HBase/"},{"name":"Serverless","slug":"Serverless","link":"/categories/Serverless/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"AWS","slug":"AWS","link":"/categories/AWS/"}]}